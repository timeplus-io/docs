# Core Settings
<!-- Autogenerated -->
All below settings are also available in table `system.settings` and [the source code](https://github.com/timeplus-io/proton/blob/develop/src/Core/Settings.h). These settings are autogenerated for timeplusd 2.5.6 on 2025-01-04.
## _tp_count_index {#_tp_count_index}

Type: int64

Internal settings for debug only. Specify which index to count its entries for mutable stream.

## _tp_enable_log_stream_expr {#_tp_enable_log_stream_expr}

Type: bool

Enable log stream analytic

## _tp_internal_system_open_sesame {#_tp_internal_system_open_sesame}

Type: bool

Control the access to system.* streams

## add_http_cors_header {#add_http_cors_header}

Type: bool

Write add http CORS header.

## additional_result_filter {#additional_result_filter}

Type: string

Additional filter expression which would be applied to query result

## additional_table_filters {#additional_table_filters}

Type: map

Additional filter expression which would be applied after reading from specified table. Syntax: \{'table1': 'expression', 'database.table2': 'expression'}

## aggregate_functions_null_for_empty {#aggregate_functions_null_for_empty}

Type: bool

Rewrite all aggregate functions in a query, adding -OrNull suffix to them

## aggregation_backfill_key_unique {#aggregation_backfill_key_unique}

Type: bool

When doing backfill data from historical store, if it is true, the aggregating key columns are unique which can help speed up the backfilling

## aggregation_in_order_max_block_bytes {#aggregation_in_order_max_block_bytes}

Type: uint64

Maximal size of block in bytes accumulated during aggregation in order of primary key. Lower block size allows to parallelize more final merge stage of aggregation.

## aggregation_memory_efficient_merge_threads {#aggregation_memory_efficient_merge_threads}

Type: uint64

Number of threads to use for merge intermediate aggregation results in memory efficient mode. When bigger, then more memory is consumed. 0 means - same as 'max_threads'.

## allow_asynchronous_read_from_io_pool_for_merge_tree {#allow_asynchronous_read_from_io_pool_for_merge_tree}

Type: bool

Use background I/O pool to read from MergeTree tables. This setting may increase performance for I/O bound queries

## allow_changing_replica_until_first_data_packet {#allow_changing_replica_until_first_data_packet}

Type: bool

Allow HedgedConnections to change replica until receiving first data packet

## allow_custom_error_code_in_throwif {#allow_custom_error_code_in_throwif}

Type: bool

Enable custom error code in function throwIf(). If true, thrown exceptions may have unexpected error codes.

## allow_ddl {#allow_ddl}

Type: bool

If it is set to true, then a user is allowed to executed DDL queries.

## allow_distributed_ddl {#allow_distributed_ddl}

Type: bool

If it is set to true, then a user is allowed to executed distributed DDL queries.

## allow_drop_detached {#allow_drop_detached}

Type: bool

Allow ALTER TABLE ... DROP DETACHED PART[ITION] ... queries

## allow_execute_multiif_columnar {#allow_execute_multiif_columnar}

Type: bool

Allow execute multiIf function columnar

## allow_experimental_alter_materialized_view_structure {#allow_experimental_alter_materialized_view_structure}

Type: bool

Allow atomic alter on Materialized views. Work in progress.

## allow_experimental_analyzer {#allow_experimental_analyzer}

Type: bool

Allow experimental analyzer

## allow_experimental_codecs {#allow_experimental_codecs}

Type: bool

If it is set to true, allow to specify experimental compression codecs (but we don't have those yet and this option does nothing).

## allow_experimental_database_materialized_postgresql {#allow_experimental_database_materialized_postgresql}

Type: bool

Allow to create database with Engine=MaterializedPostgreSQL(...).

## allow_experimental_database_replicated {#allow_experimental_database_replicated}

Type: bool

Allow to create databases with Replicated engine

## allow_experimental_funnel_functions {#allow_experimental_funnel_functions}

Type: bool

Enable experimental functions for funnel analysis.

## allow_experimental_geo_types {#allow_experimental_geo_types}

Type: bool

Allow geo data types such as Point, Ring, Polygon, MultiPolygon

## allow_experimental_hash_functions {#allow_experimental_hash_functions}

Type: bool

Enable experimental hash functions (hashid, etc)

## allow_experimental_lightweight_delete {#allow_experimental_lightweight_delete}

Type: bool

Enable lightweight DELETE mutations for mergetree tables. Work in progress

## allow_experimental_live_view {#allow_experimental_live_view}

Type: bool

Enable LIVE VIEW. Not mature enough.

## allow_experimental_nlp_functions {#allow_experimental_nlp_functions}

Type: bool

Enable experimental functions for natural language processing.

## allow_experimental_parallel_reading_from_replicas {#allow_experimental_parallel_reading_from_replicas}

Type: bool

If true, timeplusd will send a SELECT query to all replicas of a table. It will work for any kind on MergeTree table.

## allow_experimental_projection_optimization {#allow_experimental_projection_optimization}

Type: bool

Enable projection optimization when processing SELECT queries

## allow_experimental_query_deduplication {#allow_experimental_query_deduplication}

Type: bool

Experimental data deduplication for SELECT queries based on part UUIDs

## allow_experimental_window_view {#allow_experimental_window_view}

Type: bool

Enable WINDOW VIEW. Not mature enough.

## allow_hyperscan {#allow_hyperscan}

Type: bool

Allow functions that use Hyperscan library. Disable to avoid potentially long compilation times and excessive resource usage.

## allow_independent_shard_processing {#allow_independent_shard_processing}

Type: bool

When data is already sharded correctly on file system and if the aggregation is on the sharding key, we can avoid re-shuffle the data

## allow_introspection_functions {#allow_introspection_functions}

Type: bool

Allow functions for introspection of ELF and DWARF for query profiling. These functions are slow and may impose security considerations.

## allow_non_metadata_alters {#allow_non_metadata_alters}

Type: bool

Allow to execute alters which affects not only tables metadata, but also data on disk

## allow_nondeterministic_mutations {#allow_nondeterministic_mutations}

Type: bool

Allow non-deterministic functions in ALTER UPDATE/ALTER DELETE statements

## allow_nondeterministic_optimize_skip_unused_shards {#allow_nondeterministic_optimize_skip_unused_shards}

Type: bool

Allow non-deterministic functions (includes dictGet) in sharding_key for optimize_skip_unused_shards

## allow_prefetched_read_pool_for_local_filesystem {#allow_prefetched_read_pool_for_local_filesystem}

Type: bool

Prefer prefethed threadpool if all parts are on remote filesystem

## allow_prefetched_read_pool_for_remote_filesystem {#allow_prefetched_read_pool_for_remote_filesystem}

Type: bool

Prefer prefethed threadpool if all parts are on remote filesystem

## allow_push_predicate_when_subquery_contains_with {#allow_push_predicate_when_subquery_contains_with}

Type: bool

Allows push predicate when subquery contains WITH clause

## allow_settings_after_format_in_insert {#allow_settings_after_format_in_insert}

Type: bool

Allow SETTINGS after FORMAT, but note, that this is not always safe (note: this is a compatibility setting).

## allow_simdjson {#allow_simdjson}

Type: bool

Allow using simdjson library in 'JSON*' functions if AVX2 instructions are available. If disabled rapidjson will be used.

## allow_suspicious_codecs {#allow_suspicious_codecs}

Type: bool

If it is set to true, allow to specify meaningless compression codecs.

## allow_suspicious_low_cardinality_types {#allow_suspicious_low_cardinality_types}

Type: bool

In CREATE STREAM statement allows specifying LowCardinality modifier for types of small fixed size (8 or less). Enabling this may increase merge times and memory consumption.

## alter_partition_verbose_result {#alter_partition_verbose_result}

Type: bool

Output information about affected parts. Currently works only for FREEZE and ATTACH commands.

## analyze_index_with_space_filling_curves {#analyze_index_with_space_filling_curves}

Type: bool

If a table has a space-filling curve in its index, e.g. `ORDER BY morton_encode(x, y)`, and the query has conditions on its arguments, e.g. `x >= 10 AND x \<= 20 AND y >= 20 AND y \<= 30`, use the space-filling curve for index analysis.

## any_join_distinct_right_table_keys {#any_join_distinct_right_table_keys}

Type: bool

Enable old ANY JOIN logic with many-to-one left-to-right table keys mapping for all ANY JOINs. It leads to confusing not equal results for 't1 ANY LEFT JOIN t2' and 't2 ANY RIGHT JOIN t1'. ANY RIGHT JOIN needs one-to-many keys mapping to be consistent with LEFT one.

## asterisk_include_alias_columns {#asterisk_include_alias_columns}

Type: bool

Include ALIAS columns for wildcard query

## asterisk_include_materialized_columns {#asterisk_include_materialized_columns}

Type: bool

Include MATERIALIZED columns for wildcard query

## asterisk_include_reserved_columns {#asterisk_include_reserved_columns}

Type: bool

Show reserved columns on SELECT query.

## asterisk_include_tp_sn_column {#asterisk_include_tp_sn_column}

Type: bool

Include _tp_sn column for wildcard query

## async_ingest_block_timeout_ms {#async_ingest_block_timeout_ms}

Type: int64

Max duration for a block to commit before it is considered expired during async ingestion

## async_insert {#async_insert}

Type: bool

If true, data from INSERT query is stored in queue and later flushed to table in background. Makes sense only for inserts via HTTP protocol. If wait_for_async_insert is false, INSERT query is processed almost instantly, otherwise client will wait until data will be flushed to table

## async_insert_busy_timeout_ms {#async_insert_busy_timeout_ms}

Type: milliseconds

Maximum time to wait before dumping collected data per query since the first data appeared

## async_insert_max_data_size {#async_insert_max_data_size}

Type: uint64

Maximum size in bytes of unparsed data collected per query before being inserted

## async_insert_stale_timeout_ms {#async_insert_stale_timeout_ms}

Type: milliseconds

Maximum time to wait before dumping collected data per query since the last data appeared. Zero means no timeout at all

## async_insert_threads {#async_insert_threads}

Type: uint64

Maximum number of threads to actually parse and insert data in background. Zero means asynchronous mode is disabled

## async_socket_for_remote {#async_socket_for_remote}

Type: bool

Asynchronously read from socket executing remote query

## aysnc_ingest_max_outstanding_blocks {#aysnc_ingest_max_outstanding_blocks}

Type: uint64

Max outstanding blocks to be committed during async ingestion

## background_buffer_flush_schedule_pool_size {#background_buffer_flush_schedule_pool_size}

Type: uint64

Number of threads performing background flush for tables with Buffer engine. Only has meaning at server startup.

## background_common_pool_size {#background_common_pool_size}

Type: uint64

Number of threads for some lightweight tasks for replicated tables (like cleaning old parts etc.). Only has meaning at server startup.

## background_distributed_schedule_pool_size {#background_distributed_schedule_pool_size}

Type: uint64

Number of threads performing background tasks for distributed sends. Only has meaning at server startup.

## background_fetches_pool_size {#background_fetches_pool_size}

Type: uint64

Number of threads performing background fetches for replicated tables. Only has meaning at server startup.

## background_merges_mutations_concurrency_ratio {#background_merges_mutations_concurrency_ratio}

Type: float

Ratio between a number of how many operations could be processed and a number threads to process them. Only has meaning at server startup.

## background_message_broker_schedule_pool_size {#background_message_broker_schedule_pool_size}

Type: uint64

Number of threads performing background tasks for message streaming. Only has meaning at server startup.

## background_move_pool_size {#background_move_pool_size}

Type: uint64

Number of threads performing background moves for tables. Only has meaning at server startup.

## background_pool_size {#background_pool_size}

Type: uint64

Number of threads to perform merges and mutations in background. Only has meaning at server startup.

## background_schedule_pool_size {#background_schedule_pool_size}

Type: uint64

Number of threads performing background tasks for replicated tables, dns cache updates. Only has meaning at server startup.

## bool_false_representation {#bool_false_representation}

Type: string

Text to represent bool value in TSV/CSV formats.

## bool_true_representation {#bool_true_representation}

Type: string

Text to represent bool value in TSV/CSV formats.

## calculate_text_stack_trace {#calculate_text_stack_trace}

Type: bool

Calculate text stack trace in case of exceptions during query execution. This is the default. It requires symbol lookups that may slow down fuzzing tests when huge amount of wrong queries are executed. In normal cases you should not disable this option.

## cancel_http_readonly_queries_on_client_close {#cancel_http_readonly_queries_on_client_close}

Type: bool

Cancel HTTP readonly queries when a client closes the connection without waiting for response.

## cast_ipv4_ipv6_default_on_conversion_error {#cast_ipv4_ipv6_default_on_conversion_error}

Type: bool

CAST operator into IPv4, CAST operator into IPV6 type, toIPv4, toIPv6 functions will return default value instead of throwing exception on conversion error.

## cast_keep_nullable {#cast_keep_nullable}

Type: bool

CAST operator keep Nullable for result data type

## check_query_single_value_result {#check_query_single_value_result}

Type: bool

Return check query result as single 1/0 value

## check_table_dependencies {#check_table_dependencies}

Type: bool

Check that DDL query (such as DROP TABLE or RENAME) will not break dependencies

## checkpoint_interval {#checkpoint_interval}

Type: int64

Checkpoint interval in seconds. \<0: no checkpoint, 0: default interval

## checkpoint_settings {#checkpoint_settings}

Type: string

Settings which are key=value pairs and separated by semi-colon: for example: 'type=auto;storage_type=nativelog;'

## checksum_on_read {#checksum_on_read}

Type: bool

Validate checksums on reading. It is enabled by default and should be always enabled in production. Please do not expect any benefits in disabling this setting. It may only be used for experiments and benchmarks. The setting only applicable for tables of MergeTree family. Checksums are always validated for other table engines and when receiving data over network.

## cluster_for_parallel_replicas {#cluster_for_parallel_replicas}

Type: string

Cluster for a shard in which current server is located

## collect_hash_table_stats_during_aggregation {#collect_hash_table_stats_during_aggregation}

Type: bool

Enable collecting hash table statistics to optimize memory allocation

## column_names_for_schema_inference {#column_names_for_schema_inference}

Type: string

The list of column names to use in schema inference for formats without column names. The format: 'column1,column2,column3,...'

## compact_kv_stream {#compact_kv_stream}

Type: bool

Control if compact a changelog kv or versioned kv stream during query

## compile_aggregate_expressions {#compile_aggregate_expressions}

Type: bool

Compile aggregate functions to native code.

## compile_expressions {#compile_expressions}

Type: bool

Compile some scalar functions and operators to native code.

## connect_timeout {#connect_timeout}

Type: seconds

Connection timeout if there are no replicas.

## connect_timeout_with_failover_ms {#connect_timeout_with_failover_ms}

Type: milliseconds

Connection timeout for selecting first healthy replica.

## connect_timeout_with_failover_secure_ms {#connect_timeout_with_failover_secure_ms}

Type: milliseconds

Connection timeout for selecting first healthy replica (for secure connections).

## connection_pool_max_wait_ms {#connection_pool_max_wait_ms}

Type: milliseconds

The wait time when the connection pool is full.

## connections_with_failover_max_tries {#connections_with_failover_max_tries}

Type: uint64

The maximum number of attempts to connect to replicas.

## convert_query_to_cnf {#convert_query_to_cnf}

Type: bool

Convert SELECT query to CNF

## count_distinct_implementation {#count_distinct_implementation}

Type: string

What aggregate function to use for implementation of count(DISTINCT ...)

## cross_to_inner_join_rewrite {#cross_to_inner_join_rewrite}

Type: uint64

Use inner join instead of comma/cross join if there're joining expressions in the WHERE section. Values: 0 - no rewrite, 1 - apply if possible for comma/cross, 2 - force rewrite all comma joins, cross - if possible

## data_type_default_nullable {#data_type_default_nullable}

Type: bool

Data types without NULL or NOT NULL will make Nullable

## database_atomic_wait_for_drop_and_detach_synchronously {#database_atomic_wait_for_drop_and_detach_synchronously}

Type: bool

When executing DROP or DETACH TABLE in Atomic database, wait for table data to be finally dropped or detached.

## database_replicated_always_detach_permanently {#database_replicated_always_detach_permanently}

Type: bool

Execute DETACH TABLE as DETACH TABLE PERMANENTLY if database engine is Replicated

## database_replicated_initial_query_timeout_sec {#database_replicated_initial_query_timeout_sec}

Type: uint64

How long initial DDL query should wait for Replicated database to precess previous DDL queue entries

## date_time_input_format {#date_time_input_format}

Type: datetimeinputformat

Method to read DateTime from text input formats. Possible values: 'basic' and 'best_effort'.

## date_time_output_format {#date_time_output_format}

Type: datetimeoutputformat

Method to write DateTime to text output. Possible values: 'simple', 'iso', 'unix_timestamp'.

## decimal_check_overflow {#decimal_check_overflow}

Type: bool

Check overflow of decimal arithmetic/comparison operations

## deduplicate_blocks_in_dependent_materialized_views {#deduplicate_blocks_in_dependent_materialized_views}

Type: bool

Should deduplicate blocks for materialized views if the block is not a duplicate for the table. Use true to always deduplicate in dependent tables.

## default_database_engine {#default_database_engine}

Type: defaultdatabaseengine

Default database engine.

## default_hash_join {#default_hash_join}

Type: hashjointype

Control which hash join is used for a streaming join

## default_hash_table {#default_hash_table}

Type: hashtabletype

Control which hash table is used for a streaming query like join, aggregation

## default_max_bytes_in_join {#default_max_bytes_in_join}

Type: uint64

Maximum size of right-side table if limit is required but max_bytes_in_join is not set.

## describe_extend_object_types {#describe_extend_object_types}

Type: bool

Deduce concrete type of columns of type Object in DESCRIBE query

## describe_include_subcolumns {#describe_include_subcolumns}

Type: bool

If true, subcolumns of all table columns will be included into result of DESCRIBE query

## distinct_overflow_mode {#distinct_overflow_mode}

Type: overflowmode

What to do when the limit is exceeded.

## distributed_aggregation_memory_efficient {#distributed_aggregation_memory_efficient}

Type: bool

Is the memory-saving mode of distributed aggregation enabled.

## distributed_connections_pool_size {#distributed_connections_pool_size}

Type: uint64

Maximum number of connections with one remote server in the pool.

## distributed_ddl_entry_format_version {#distributed_ddl_entry_format_version}

Type: uint64

Version of DDL entry to write into ZooKeeper

## distributed_ddl_output_mode {#distributed_ddl_output_mode}

Type: distributedddloutputmode

Format of distributed DDL query result

## distributed_ddl_task_timeout {#distributed_ddl_task_timeout}

Type: int64

Timeout for DDL query responses from all hosts in cluster. If a ddl request has not been performed on all hosts, a response will contain a timeout error and a request will be executed in an async mode. Negative value means infinite. Zero means async mode.

## distributed_directory_monitor_batch_inserts {#distributed_directory_monitor_batch_inserts}

Type: bool

Should StorageDistributed DirectoryMonitors try to batch individual inserts into bigger ones.

## distributed_directory_monitor_max_sleep_time_ms {#distributed_directory_monitor_max_sleep_time_ms}

Type: milliseconds

Maximum sleep time for StorageDistributed DirectoryMonitors, it limits exponential growth too.

## distributed_directory_monitor_sleep_time_ms {#distributed_directory_monitor_sleep_time_ms}

Type: milliseconds

Sleep time for StorageDistributed DirectoryMonitors, in case of any errors delay grows exponentially.

## distributed_directory_monitor_split_batch_on_failure {#distributed_directory_monitor_split_batch_on_failure}

Type: bool

Should StorageDistributed DirectoryMonitors try to split batch into smaller in case of failures.

## distributed_group_by_no_merge {#distributed_group_by_no_merge}

Type: uint64

If 1, Do not merge aggregation states from different servers for distributed queries (shards will process query up to the Complete stage, initiator just proxies the data from the shards). If 2 the initiator will apply ORDER BY and LIMIT stages (it is not in case when shard process query up to the Complete stage)

## distributed_product_mode {#distributed_product_mode}

Type: distributedproductmode

How are distributed subqueries performed inside IN or JOIN sections?

## distributed_push_down_limit {#distributed_push_down_limit}

Type: uint64

If 1, LIMIT will be applied on each shard separatelly. Usually you don't need to use it, since this will be done automatically if it is possible, i.e. for simple query SELECT FROM LIMIT.

## distributed_replica_error_cap {#distributed_replica_error_cap}

Type: uint64

Max number of errors per replica, prevents piling up an incredible amount of errors if replica was offline for some time and allows it to be reconsidered in a shorter amount of time.

## distributed_replica_error_half_life {#distributed_replica_error_half_life}

Type: seconds

Time period reduces replica error counter by 2 times.

## distributed_replica_max_ignored_errors {#distributed_replica_max_ignored_errors}

Type: uint64

Number of errors that will be ignored while choosing replicas

## do_not_merge_across_partitions_select_final {#do_not_merge_across_partitions_select_final}

Type: bool

Merge parts only in one partition in select final

## drain_timeout {#drain_timeout}

Type: seconds

Timeout for draining remote connections, -1 means synchronous drain w/o ignoring errors

## emit_during_backfill {#emit_during_backfill}

Type: bool

Enable emit intermediate aggr result during backfill historical data

## empty_result_for_aggregation_by_constant_keys_on_empty_set {#empty_result_for_aggregation_by_constant_keys_on_empty_set}

Type: bool

Return empty result when aggregating by constant keys on empty set.

## empty_result_for_aggregation_by_empty_set {#empty_result_for_aggregation_by_empty_set}

Type: bool

Return empty result when aggregating without keys on empty set.

## enable_backfill_from_historical_store {#enable_backfill_from_historical_store}

Type: bool

Enable backfill data from historical data store

## enable_concurrent_ingest {#enable_concurrent_ingest}

Type: bool

Control if concurrent ingest is enabled. If it is true, `max_insert_thread` will be honored in a best way to optimize ingest throughput

## enable_dependency_check {#enable_dependency_check}

Type: bool

Enable the dependency check of view/materialized view

## enable_early_constant_folding {#enable_early_constant_folding}

Type: bool

Enable query optimization where we analyze function and subqueries results and rewrite query if there're constants there

## enable_extended_results_for_datetime_functions {#enable_extended_results_for_datetime_functions}

Type: bool

Enable date functions like toLastDayOfMonth return Date32 results (instead of Date results) for Date32/DateTime64 arguments.

## enable_filesystem_cache {#enable_filesystem_cache}

Type: bool

Use cache for remote filesystem. This setting does not turn on/off cache for disks (must me done via disk config), but allows to bypass cache for some queries if intended

## enable_filesystem_cache_log {#enable_filesystem_cache_log}

Type: bool

Allows to record the filesystem caching log for each query

## enable_filesystem_cache_on_write_operations {#enable_filesystem_cache_on_write_operations}

Type: bool

Write into cache on write operations. To actually work this setting requires be added to disk config too

## enable_filesystem_read_prefetches_log {#enable_filesystem_read_prefetches_log}

Type: bool

Log to system.filesystem prefetch_log during query. Should be used only for testing or debugging, not recommended to be turned on by default

## enable_global_with_statement {#enable_global_with_statement}

Type: bool

Propagate WITH statements to UNION queries and all subqueries

## enable_http_compression {#enable_http_compression}

Type: bool

Compress the result if the client over HTTP said that it understands data compressed by gzip or deflate.

## enable_idempotent_processing {#enable_idempotent_processing}

Type: bool

Enable idempotent processing for streaming query. Only work with idempotent ingestion

## enable_light_ingest {#enable_light_ingest}

Type: bool

Light ingest is inserting partial columns of a table

## enable_memory_bound_merging_of_aggregation_results {#enable_memory_bound_merging_of_aggregation_results}

Type: bool

Enable memory bound merging strategy for aggregation. Set it to true only if all nodes of your clusters have versions >= 22.12.

## enable_multiple_prewhere_read_steps {#enable_multiple_prewhere_read_steps}

Type: bool

Move more conditions from WHERE to PREWHERE and do reads from disk and filtering in multiple steps if there are multiple conditions combined with AND

## enable_optimize_predicate_expression {#enable_optimize_predicate_expression}

Type: bool

If it is set to true, optimize predicates to subqueries.

## enable_optimize_predicate_expression_to_final_subquery {#enable_optimize_predicate_expression_to_final_subquery}

Type: bool

Allow push predicate to final subquery.

## enable_positional_arguments {#enable_positional_arguments}

Type: bool

Enable positional arguments in ORDER BY, GROUP BY and LIMIT BY

## enable_query_pipe {#enable_query_pipe}

Type: bool

Enable query pipe

## enable_s3_requests_logging {#enable_s3_requests_logging}

Type: bool

Enable very explicit logging of S3 requests. Makes sense for debug only.

## enable_scalar_subquery_optimization {#enable_scalar_subquery_optimization}

Type: bool

If it is set to true, prevent scalar subqueries from (de)serializing large scalar values and possibly avoid running the same subquery more than once.

## enable_sharing_sets_for_mutations {#enable_sharing_sets_for_mutations}

Type: bool

Allow sharing set objects build for IN subqueries between different tasks of the same mutation. This reduces memory usage and CPU consumption

## enable_software_prefetch_in_aggregation {#enable_software_prefetch_in_aggregation}

Type: bool

Enable use of software prefetch in aggregation

## enable_unaligned_array_join {#enable_unaligned_array_join}

Type: bool

Allow ARRAY JOIN with multiple arrays that have different sizes. When this settings is enabled, arrays will be resized to the longest one.

## enforce_append_only {#enforce_append_only}

Type: bool

For changelog storage, enforce query it as append only storage

## engine_file_allow_create_multiple_files {#engine_file_allow_create_multiple_files}

Type: bool

Enables or disables creating a new file on each insert in file engine tables if format has suffix.

## engine_file_empty_if_not_exists {#engine_file_empty_if_not_exists}

Type: bool

Allows to select data from a file engine table without file

## engine_file_truncate_on_insert {#engine_file_truncate_on_insert}

Type: bool

Enables or disables truncate before insert in file engine tables

## eps {#eps}

Type: float

control the random stream eps in query time, the default value is -1 which means it will use default eps defined when creating the random stream. If it is 0 means no limit.

## errors_output_format {#errors_output_format}

Type: string

Method to write Errors to text output.

## exec_mode {#exec_mode}

Type: executemode

Control query execute mode

## experimental_query_deduplication_send_all_part_uuids {#experimental_query_deduplication_send_all_part_uuids}

Type: bool

If false only part UUIDs for currently moving parts are sent. If true all read part UUIDs are sent (useful only for testing).

## external_storage_connect_timeout_sec {#external_storage_connect_timeout_sec}

Type: uint64

Connect timeout in seconds. Now supported only for MySQL

## external_storage_max_read_bytes {#external_storage_max_read_bytes}

Type: uint64

Limit maximum number of bytes when table with external engine should flush history data. Now supported only for MySQL table engine, database engine, dictionary. If equal to 0, this setting is disabled

## external_storage_max_read_rows {#external_storage_max_read_rows}

Type: uint64

Limit maximum number of rows when table with external engine should flush history data. Now supported only for MySQL table engine, database engine, dictionary. If equal to 0, this setting is disabled

## external_storage_rw_timeout_sec {#external_storage_rw_timeout_sec}

Type: uint64

Read/write timeout in seconds. Now supported only for MySQL

## external_table_functions_use_nulls {#external_table_functions_use_nulls}

Type: bool

If it is set to true, external table functions will implicitly use Nullable type if needed. Otherwise NULLs will be substituted with default values. Currently supported only by 'mysql', 'postgresql' and 'odbc' table functions.

## external_table_strict_query {#external_table_strict_query}

Type: bool

If it is set to true, transforming expression to local filter is forbidden for queries to external tables.

## extract_kvp_max_pairs_per_row {#extract_kvp_max_pairs_per_row}

Type: uint64

Max number pairs that can be produced by extractKeyValuePairs function. Used to safeguard against consuming too much memory.

## extremes {#extremes}

Type: bool

Calculate minimums and maximums of the result columns. They can be output in JSON-formats.

## fallback_to_stale_replicas_for_distributed_queries {#fallback_to_stale_replicas_for_distributed_queries}

Type: bool

Suppose max_replica_delay_for_distributed_queries is set and all replicas for the queried table are stale. If this setting is enabled, the query will be performed anyway, otherwise the error will be reported.

## fetch_buffer_size {#fetch_buffer_size}

Type: uint64

Fetch buffer per query

## filesystem_cache_max_download_size {#filesystem_cache_max_download_size}

Type: uint64

Max remote filesystem cache size that can be downloaded by a single query

## filesystem_prefetch_max_memory_usage {#filesystem_prefetch_max_memory_usage}

Type: uint64

Maximum memory usage for prefetches. Zero means unlimited

## filesystem_prefetch_min_bytes_for_single_read_task {#filesystem_prefetch_min_bytes_for_single_read_task}

Type: uint64

Do not parallelize within one file read less than this amount of bytes. E.g. one reader will not receive a read task of size less than this amount. This setting is recommended to avoid spikes of time for aws getObject requests to aws

## filesystem_prefetch_step_bytes {#filesystem_prefetch_step_bytes}

Type: uint64

Prefetch step in bytes. Zero means `auto` - approximately the best prefetch step will be auto deduced, but might not be 100% the best. The actual value might be different because of setting filesystem_prefetch_min_bytes_for_single_read_task

## filesystem_prefetch_step_marks {#filesystem_prefetch_step_marks}

Type: uint64

Prefetch step in marks. Zero means `auto` - approximately the best prefetch step will be auto deduced, but might not be 100% the best. The actual value might be different because of setting filesystem_prefetch_min_bytes_for_single_read_task

## filesystem_prefetches_limit {#filesystem_prefetches_limit}

Type: uint64

Maximum number of prefetches. Zero means unlimited. A setting `filesystem_prefetches_max_memory_usage` is more recommended if you want to limit the number of prefetches

## flatten_nested {#flatten_nested}

Type: bool

If true, columns of type Nested will be flatten to separate array columns instead of one array of tuples

## force_aggregation_in_order {#force_aggregation_in_order}

Type: bool

Force use of aggregation in order on remote nodes during distributed aggregation. PLEASE, NEVER CHANGE THIS SETTING VALUE MANUALLY!

## force_backfill_in_order {#force_backfill_in_order}

Type: bool

Requires backfill data in order

## force_data_skipping_indices {#force_data_skipping_indices}

Type: string

Comma separated list of strings or literals with the name of the data skipping indices that should be used during query execution, otherwise an exception will be thrown.

## force_drop_big_stream {#force_drop_big_stream}

Type: bool

When enabled, the query can directly force drop big stream greater than configured `max_[stream/partition]_size_to_drop`

## force_full_scan {#force_full_scan}

Type: bool

When enabled, the query will do full scan of the data directly instead of using indexes to find data. Used by mutable stream

## force_grouping_standard_compatibility {#force_grouping_standard_compatibility}

Type: bool

Make GROUPING function to return 1 when argument is not used as an aggregation key

## force_index_by_date {#force_index_by_date}

Type: bool

Throw an exception if there is a partition key in a table, and it is not used.

## force_optimize_projection {#force_optimize_projection}

Type: bool

If projection optimization is enabled, SELECT queries need to use projection

## force_optimize_skip_unused_shards {#force_optimize_skip_unused_shards}

Type: uint64

Throw an exception if unused shards cannot be skipped (1 - throw only if the table has the sharding key, 2 - always throw.

## force_optimize_skip_unused_shards_nesting {#force_optimize_skip_unused_shards_nesting}

Type: uint64

Same as force_optimize_skip_unused_shards, but accept nesting level until which it will work.

## force_primary_key {#force_primary_key}

Type: bool

Throw an exception if there is primary key in a table, and it is not used.

## force_refresh_schema {#force_refresh_schema}

Type: bool

If set to true, force fetching the schema from schema registry (otherwise the cached schema, if exists, will be used).

## force_remove_data_recursively_on_drop {#force_remove_data_recursively_on_drop}

Type: bool

Recursively remove data on DROP query. Avoids 'Directory not empty' error, but may silently remove detached data

## format_avro_schema_registry_url {#format_avro_schema_registry_url}

Type: uri

For AvroConfluent format: Confluent Schema Registry URL.

## format_binary_max_string_size {#format_binary_max_string_size}

Type: uint64

The maximum allowed size for String in RowBinary format. It prevents allocating large amount of memory in case of corrupted data. 0 means there is no limit

## format_capn_proto_enum_comparising_mode {#format_capn_proto_enum_comparising_mode}

Type: enumcomparingmode

How to map timeplusd Enum and CapnProto Enum

## format_capn_proto_use_autogenerated_schema {#format_capn_proto_use_autogenerated_schema}

Type: bool

Use autogenerated CapnProto schema when format_schema is not set

## format_csv_allow_double_quotes {#format_csv_allow_double_quotes}

Type: bool

If it is set to true, allow strings in double quotes.

## format_csv_allow_single_quotes {#format_csv_allow_single_quotes}

Type: bool

If it is set to true, allow strings in single quotes.

## format_csv_delimiter {#format_csv_delimiter}

Type: char

The character to be considered as a delimiter in CSV data. If setting with a string, a string has to have a length of 1.

## format_csv_null_representation {#format_csv_null_representation}

Type: string

Custom NULL representation in CSV format

## format_custom_escaping_rule {#format_custom_escaping_rule}

Type: escapingrule

Field escaping rule (for CustomSeparated format)

## format_custom_field_delimiter {#format_custom_field_delimiter}

Type: string

Delimiter between fields (for CustomSeparated format)

## format_custom_result_after_delimiter {#format_custom_result_after_delimiter}

Type: string

Suffix after result set (for CustomSeparated format)

## format_custom_result_before_delimiter {#format_custom_result_before_delimiter}

Type: string

Prefix before result set (for CustomSeparated format)

## format_custom_row_after_delimiter {#format_custom_row_after_delimiter}

Type: string

Delimiter after field of the last column (for CustomSeparated format)

## format_custom_row_before_delimiter {#format_custom_row_before_delimiter}

Type: string

Delimiter before field of the first column (for CustomSeparated format)

## format_custom_row_between_delimiter {#format_custom_row_between_delimiter}

Type: string

Delimiter between rows (for CustomSeparated format)

## format_protobuf_use_autogenerated_schema {#format_protobuf_use_autogenerated_schema}

Type: bool

Use autogenerated Protobuf when format_schema is not set

## format_regexp {#format_regexp}

Type: string

Regular expression (for Regexp format)

## format_regexp_escaping_rule {#format_regexp_escaping_rule}

Type: escapingrule

Field escaping rule (for Regexp format)

## format_regexp_skip_unmatched {#format_regexp_skip_unmatched}

Type: bool

Skip lines unmatched by regular expression (for Regexp format

## format_schema {#format_schema}

Type: string

Schema identifier (used by schema-based formats)

## format_template_resultset {#format_template_resultset}

Type: string

Path to file which contains format string for result set (for Template format)

## format_template_row {#format_template_row}

Type: string

Path to file which contains format string for rows (for Template format)

## format_template_rows_between_delimiter {#format_template_rows_between_delimiter}

Type: string

Delimiter between rows (for Template format)

## format_tsv_null_representation {#format_tsv_null_representation}

Type: string

Custom NULL representation in TSV format

## formatdatetime_parsedatetime_m_is_month_name {#formatdatetime_parsedatetime_m_is_month_name}

Type: bool

Formatter '%M' in function 'formatDateTime' produces the month name instead of minutes.

## fsync_metadata {#fsync_metadata}

Type: bool

Do fsync after changing metadata for tables and databases (.sql files). Could be disabled in case of poor latency on server with high load of DDL queries and high load of disk subsystem.

## function_implementation {#function_implementation}

Type: string

Choose function implementation for specific target or variant (experimental). If empty enable all of them.

## function_range_max_elements_in_block {#function_range_max_elements_in_block}

Type: uint64

Maximum number of values generated by function 'range' per block of data (sum of array sizes for every row in a block, see also 'max_block_size' and 'min_insert_block_size_rows'). It is a safety threshold.

## glob_expansion_max_elements {#glob_expansion_max_elements}

Type: uint64

Maximum number of allowed addresses (For external storages, table functions, etc).

## grace_hash_join_initial_buckets {#grace_hash_join_initial_buckets}

Type: uint64

Initial number of grace hash join buckets

## grace_hash_join_max_buckets {#grace_hash_join_max_buckets}

Type: uint64

Limit on the number of grace hash join buckets

## group_by_overflow_mode {#group_by_overflow_mode}

Type: overflowmodegroupby

What to do when the limit is exceeded.

## group_by_two_level_threshold {#group_by_two_level_threshold}

Type: uint64

From what number of keys, a two-level aggregation starts. 0 - the threshold is not set.

## group_by_two_level_threshold_bytes {#group_by_two_level_threshold_bytes}

Type: uint64

From what size of the aggregation state in bytes, a two-level aggregation begins to be used. 0 - the threshold is not set. Two-level aggregation is used when at least one of the thresholds is triggered.

## hdfs_create_new_file_on_insert {#hdfs_create_new_file_on_insert}

Type: bool

Enables or disables creating a new file on each insert in hdfs engine tables

## hdfs_replication {#hdfs_replication}

Type: uint64

The actual number of replications can be specified when the hdfs file is created.

## hdfs_truncate_on_insert {#hdfs_truncate_on_insert}

Type: bool

Enables or disables truncate before insert in s3 engine tables

## hedged_connection_timeout_ms {#hedged_connection_timeout_ms}

Type: milliseconds

Connection timeout for establishing connection with replica for Hedged requests

## hsts_max_age {#hsts_max_age}

Type: uint64

Expired time for hsts. 0 means disable HSTS.

## http_connection_timeout {#http_connection_timeout}

Type: seconds

HTTP connection timeout.

## http_headers_progress_interval_ms {#http_headers_progress_interval_ms}

Type: uint64

Do not send HTTP headers X-Proton-Progress more frequently than at each specified interval.

## http_max_field_name_size {#http_max_field_name_size}

Type: uint64

Maximum length of field name in HTTP header

## http_max_field_value_size {#http_max_field_value_size}

Type: uint64

Maximum length of field value in HTTP header

## http_max_fields {#http_max_fields}

Type: uint64

Maximum number of fields in HTTP header

## http_max_multipart_form_data_size {#http_max_multipart_form_data_size}

Type: uint64

Limit on size of multipart/form-data content. This setting cannot be parsed from URL parameters and should be set in user profile. Note that content is parsed and external tables are created in memory before start of query execution. And this is the only limit that has effect on that stage (limits on max memory usage and max execution time have no effect while reading HTTP form data).

## http_max_tries {#http_max_tries}

Type: uint64

Max attempts to read via http.

## http_max_uri_size {#http_max_uri_size}

Type: uint64

Maximum URI length of HTTP request

## http_native_compression_disable_checksumming_on_decompress {#http_native_compression_disable_checksumming_on_decompress}

Type: bool

If you uncompress the POST data from the client compressed by the native format, do not check the checksum.

## http_receive_timeout {#http_receive_timeout}

Type: seconds

HTTP receive timeout

## http_retry_initial_backoff_ms {#http_retry_initial_backoff_ms}

Type: uint64

Min milliseconds for backoff, when retrying read via http

## http_retry_max_backoff_ms {#http_retry_max_backoff_ms}

Type: uint64

Max milliseconds for backoff, when retrying read via http

## http_send_timeout {#http_send_timeout}

Type: seconds

HTTP send timeout

## http_skip_not_found_url_for_globs {#http_skip_not_found_url_for_globs}

Type: bool

Skip url's for globs with HTTP_NOT_FOUND error

## http_zlib_compression_level {#http_zlib_compression_level}

Type: int64

Compression level - used if the client on HTTP said that it understands data compressed by gzip or deflate.

## idempotent_id {#idempotent_id}

Type: string

Idempotent ID for INSERT query.

## idle_connection_timeout {#idle_connection_timeout}

Type: uint64

Close idle TCP connections after specified number of seconds.

## ignore_data_skipping_indices {#ignore_data_skipping_indices}

Type: string

Comma separated list of strings or literals with the name of the data skipping indices that should be excluded during query execution.

## include_internal_streams {#include_internal_streams}

Type: bool

Show internal streams on SHOW streams query.

## input_format_allow_errors_num {#input_format_allow_errors_num}

Type: uint64

Maximum absolute amount of errors while reading text formats (like CSV, TSV). In case of error, if at least absolute or relative amount of errors is lower than corresponding value, will skip until next line and continue.

## input_format_allow_errors_ratio {#input_format_allow_errors_ratio}

Type: float

Maximum relative amount of errors while reading text formats (like CSV, TSV). In case of error, if at least absolute or relative amount of errors is lower than corresponding value, will skip until next line and continue.

## input_format_allow_seeks {#input_format_allow_seeks}

Type: bool

Allow seeks while reading in ORC/Parquet/Arrow input formats

## input_format_arrow_allow_missing_columns {#input_format_arrow_allow_missing_columns}

Type: bool

Allow missing columns while reading Arrow input formats

## input_format_arrow_case_insensitive_column_matching {#input_format_arrow_case_insensitive_column_matching}

Type: bool

Ignore case when matching Arrow columns with CH columns.

## input_format_arrow_import_nested {#input_format_arrow_import_nested}

Type: bool

Allow to insert array of structs into Nested table in Arrow input format.

## input_format_arrow_skip_columns_with_unsupported_types_in_schema_inference {#input_format_arrow_skip_columns_with_unsupported_types_in_schema_inference}

Type: bool

Skip columns with unsupported types while schema inference for format Arrow

## input_format_avro_allow_missing_fields {#input_format_avro_allow_missing_fields}

Type: bool

For Avro/AvroConfluent format: when field is not found in schema use default value instead of error

## input_format_avro_null_as_default {#input_format_avro_null_as_default}

Type: bool

For Avro/AvroConfluent format: insert default in case of null and non Nullable column

## input_format_capn_proto_skip_fields_with_unsupported_types_in_schema_inference {#input_format_capn_proto_skip_fields_with_unsupported_types_in_schema_inference}

Type: bool

Skip columns with unsupported types while schema inference for format CapnProto

## input_format_csv_arrays_as_nested_csv {#input_format_csv_arrays_as_nested_csv}

Type: bool

When reading array from CSV, expect that its elements were serialized in nested CSV and then put into string. Example: "[""Hello"", ""world"", ""42"""" TV""]". Braces around array can be omitted.

## input_format_csv_empty_as_default {#input_format_csv_empty_as_default}

Type: bool

Treat empty fields in CSV input as default values.

## input_format_csv_enum_as_number {#input_format_csv_enum_as_number}

Type: bool

Treat inserted enum values in CSV formats as enum indices \N

## input_format_csv_skip_first_lines {#input_format_csv_skip_first_lines}

Type: uint64

Skip specified amount of lines in the beginning of data in CSV format

## input_format_csv_use_best_effort_in_schema_inference {#input_format_csv_use_best_effort_in_schema_inference}

Type: bool

Use some tweaks and heuristics to infer schema in CSV format

## input_format_defaults_for_omitted_fields {#input_format_defaults_for_omitted_fields}

Type: bool

For input data calculate default expressions for omitted fields (it works for JSONEachRow, -WithNames, -WithNamesAndTypes formats).

## input_format_hive_text_collection_items_delimiter {#input_format_hive_text_collection_items_delimiter}

Type: char

Delimiter between collection(array or map) items in Hive Text File

## input_format_hive_text_fields_delimiter {#input_format_hive_text_fields_delimiter}

Type: char

Delimiter between fields in Hive Text File

## input_format_hive_text_map_keys_delimiter {#input_format_hive_text_map_keys_delimiter}

Type: char

Delimiter between a pair of map key/values in Hive Text File

## input_format_import_nested_json {#input_format_import_nested_json}

Type: bool

The map nested JSON data to nested tables (it works for JSONEachRow format).

## input_format_ipv4_default_on_conversion_error {#input_format_ipv4_default_on_conversion_error}

Type: bool

Deserialization of IPv4 will use default values instead of throwing exception on conversion error.

## input_format_ipv6_default_on_conversion_error {#input_format_ipv6_default_on_conversion_error}

Type: bool

Deserialization of IPV6 will use default values instead of throwing exception on conversion error.

## input_format_json_read_bools_as_numbers {#input_format_json_read_bools_as_numbers}

Type: bool

Allow to parse bools as numbers in JSON input formats

## input_format_json_try_infer_numbers_from_strings {#input_format_json_try_infer_numbers_from_strings}

Type: bool

Try to infer numbers from string fields while schema inference

## input_format_max_rows_to_read_for_schema_inference {#input_format_max_rows_to_read_for_schema_inference}

Type: uint64

The maximum rows of data to read for automatic schema inference

## input_format_msgpack_number_of_columns {#input_format_msgpack_number_of_columns}

Type: uint64

The number of columns in inserted MsgPack data. Used for automatic schema inference from data.

## input_format_null_as_default {#input_format_null_as_default}

Type: bool

Initialize null fields with default values if the data type of this field is not nullable and it is supported by the input format

## input_format_orc_allow_missing_columns {#input_format_orc_allow_missing_columns}

Type: bool

Allow missing columns while reading ORC input formats

## input_format_orc_case_insensitive_column_matching {#input_format_orc_case_insensitive_column_matching}

Type: bool

Ignore case when matching ORC columns with CH columns.

## input_format_orc_import_nested {#input_format_orc_import_nested}

Type: bool

Allow to insert array of structs into Nested table in ORC input format.

## input_format_orc_row_batch_size {#input_format_orc_row_batch_size}

Type: int64

Batch size when reading ORC stripes.

## input_format_orc_skip_columns_with_unsupported_types_in_schema_inference {#input_format_orc_skip_columns_with_unsupported_types_in_schema_inference}

Type: bool

Skip columns with unsupported types while schema inference for format ORC

## input_format_parallel_parsing {#input_format_parallel_parsing}

Type: bool

Enable parallel parsing for some data formats.

## input_format_parquet_allow_missing_columns {#input_format_parquet_allow_missing_columns}

Type: bool

Allow missing columns while reading Parquet input formats

## input_format_parquet_case_insensitive_column_matching {#input_format_parquet_case_insensitive_column_matching}

Type: bool

Ignore case when matching Parquet columns with CH columns.

## input_format_parquet_import_nested {#input_format_parquet_import_nested}

Type: bool

Allow to insert array of structs into Nested table in Parquet input format.

## input_format_parquet_max_block_size {#input_format_parquet_max_block_size}

Type: uint64

Max block size for parquet reader.

## input_format_parquet_preserve_order {#input_format_parquet_preserve_order}

Type: bool

Avoid reordering rows when reading from Parquet files. Usually makes it much slower.

## input_format_parquet_skip_columns_with_unsupported_types_in_schema_inference {#input_format_parquet_skip_columns_with_unsupported_types_in_schema_inference}

Type: bool

Skip columns with unsupported types while schema inference for format Parquet

## input_format_protobuf_flatten_google_wrappers {#input_format_protobuf_flatten_google_wrappers}

Type: bool

Enable Google wrappers for regular non-nested columns, e.g. google.protobuf.StringValue 'str' for String column 'str'. For Nullable columns empty wrappers are recognized as defaults, and missing as nulls

## input_format_protobuf_skip_fields_with_unsupported_types_in_schema_inference {#input_format_protobuf_skip_fields_with_unsupported_types_in_schema_inference}

Type: bool

Skip fields with unsupported types while schema inference for format Protobuf

## input_format_record_errors_file_path {#input_format_record_errors_file_path}

Type: string

Path of the file used to record errors while reading text formats (CSV, TSV).

## input_format_skip_unknown_fields {#input_format_skip_unknown_fields}

Type: bool

Skip columns with unknown names from input data (it works for JSONEachRow, -WithNames, -WithNamesAndTypes and TSKV formats).

## input_format_try_infer_dates {#input_format_try_infer_dates}

Type: bool

Try to infer dates from string fields while schema inference in text formats

## input_format_try_infer_datetimes {#input_format_try_infer_datetimes}

Type: bool

Try to infer datetimes from string fields while schema inference in text formats

## input_format_try_infer_integers {#input_format_try_infer_integers}

Type: bool

Try to infer numbers from string fields while schema inference in text formats

## input_format_tsv_empty_as_default {#input_format_tsv_empty_as_default}

Type: bool

Treat empty fields in TSV input as default values.

## input_format_tsv_enum_as_number {#input_format_tsv_enum_as_number}

Type: bool

Treat inserted enum values in TSV formats as enum indices \N

## input_format_tsv_skip_first_lines {#input_format_tsv_skip_first_lines}

Type: uint64

Skip specified amount of lines in the beginning of data in TSV format

## input_format_tsv_use_best_effort_in_schema_inference {#input_format_tsv_use_best_effort_in_schema_inference}

Type: bool

Use some tweaks and heuristics to infer schema in TSV format

## input_format_values_accurate_types_of_literals {#input_format_values_accurate_types_of_literals}

Type: bool

For Values format: when parsing and interpreting expressions using template, check actual type of literal to avoid possible overflow and precision issues.

## input_format_values_deduce_templates_of_expressions {#input_format_values_deduce_templates_of_expressions}

Type: bool

For Values format: if the field could not be parsed by streaming parser, run SQL parser, deduce template of the SQL expression, try to parse all rows using template and then interpret expression for all rows.

## input_format_values_interpret_expressions {#input_format_values_interpret_expressions}

Type: bool

For Values format: if the field could not be parsed by streaming parser, run SQL parser and try to interpret it as SQL expression.

## input_format_with_names_use_header {#input_format_with_names_use_header}

Type: bool

For -WithNames input formats this controls whether format parser is to assume that column data appear in the input exactly as they are specified in the header.

## input_format_with_types_use_header {#input_format_with_types_use_header}

Type: bool

For -WithNamesAndTypes input formats this controls whether format parser should check if data types from the input match data types from the header.

## insert_allow_materialized_columns {#insert_allow_materialized_columns}

Type: bool

If setting is enabled, Allow materialized columns in INSERT.

## insert_block_timeout_ms {#insert_block_timeout_ms}

Type: int64

The maximum time in milliseconds for constructing a block for insertion, i.e. batching. Increasing the value gives greater possibility to create bigger blocks (limited by max_insert_block_bytes and max_insert_block_size), but also increases latency. Negative numbers means no timeout.

## insert_deduplicate {#insert_deduplicate}

Type: bool

For INSERT queries in the replicated table, specifies that deduplication of insertings blocks should be performed

## insert_deduplication_token {#insert_deduplication_token}

Type: string

If not empty, used for duplicate detection instead of data digest

## insert_distributed_one_random_shard {#insert_distributed_one_random_shard}

Type: bool

If setting is enabled, inserting into distributed table will choose a random shard to write when there is no sharding key

## insert_distributed_sync {#insert_distributed_sync}

Type: bool

If setting is enabled, insert query into distributed waits until data will be sent to all nodes in cluster.

## insert_distributed_timeout {#insert_distributed_timeout}

Type: uint64

Timeout for insert query into distributed. Setting is used only with insert_distributed_sync enabled. Zero value means no timeout.

## insert_null_as_default {#insert_null_as_default}

Type: bool

Insert DEFAULT values instead of NULL in INSERT SELECT (UNION ALL)

## insert_quorum {#insert_quorum}

Type: uint64

For INSERT queries in the replicated table, wait writing for the specified number of replicas and linearize the addition of the data. 0 - disabled.

## insert_quorum_parallel {#insert_quorum_parallel}

Type: bool

For quorum INSERT queries - enable to make parallel inserts without linearizability

## insert_quorum_timeout {#insert_quorum_timeout}

Type: milliseconds



## insert_shard_id {#insert_shard_id}

Type: uint64

If non zero, when insert into a distributed table, the data will be inserted into the shard `insert_shard_id` synchronously. Possible values range from 1 to `shards_number` of corresponding distributed table

## insert_timeout_ms {#insert_timeout_ms}

Type: int64

The maximum duration for appending data to nativelog before it is considered expired.

## interactive_delay {#interactive_delay}

Type: uint64

The interval in microseconds to check if the request is cancelled, and to send progress info.

## is_internal {#is_internal}

Type: bool

Control the statistics of select query

## javascript_max_memory_bytes {#javascript_max_memory_bytes}

Type: uint64

Maximum heap size of javascript UDA/UDF in bytes

## javascript_uda_max_concurrency {#javascript_uda_max_concurrency}

Type: uint64

Control the concurrency of JavaScript UDA in a query

## join_algorithm {#join_algorithm}

Type: joinalgorithm

Specify join algorithm.

## join_any_take_last_row {#join_any_take_last_row}

Type: bool

When disabled (default) ANY JOIN will take the first found row for a key. When enabled, it will take the last row seen if there are multiple rows for the same key.

## join_buffered_data_block_size {#join_buffered_data_block_size}

Type: uint64

For streaming join, when buffered data in memory, the data block size directs to merge small data blocks to form bigger ones to improve memory efficiency. 0 means disable merging small data blocks.

## join_default_strictness {#join_default_strictness}

Type: joinstrictness

Set default strictness in JOIN query. Possible values: empty string, 'ANY', 'ALL'. If empty, query without strictness will throw exception.

## join_latency_threshold {#join_latency_threshold}

Type: int64

Control when to start join left stream with right stream for alignment join scenarios. Zero means system picked threshold

## join_max_buffered_bytes {#join_max_buffered_bytes}

Type: uint64

Max buffered bytes for stream to stream join

## join_on_disk_max_files_to_merge {#join_on_disk_max_files_to_merge}

Type: uint64

For MergeJoin on disk set how much files it's allowed to sort simultaneously. Then this value bigger then more memory used and then less disk I/O needed. Minimum is 2.

## join_overflow_mode {#join_overflow_mode}

Type: overflowmode

What to do when the limit is exceeded.

## join_quiesce_threshold_ms {#join_quiesce_threshold_ms}

Type: int64

For streaming join, when left or right stream is in quiesce, the maximum time to wait before the join.

## join_use_nulls {#join_use_nulls}

Type: bool

Use NULLs for non-joined rows of outer JOINs for types that can be inside Nullable. If false, use default value of corresponding columns data type.

## joined_subquery_requires_alias {#joined_subquery_requires_alias}

Type: bool

Force joined subqueries and table functions to have aliases for correct name qualification.

## kafka_client_queued_max_bytes {#kafka_client_queued_max_bytes}

Type: int64

Maximum bytes per topic partition to buffer on librdkafka client queue 

## kafka_client_queued_min_message {#kafka_client_queued_min_message}

Type: int64

Minimum number of messages per topic partition to buffer on librdkafka client queue 

## kafka_fetch_max_bytes {#kafka_fetch_max_bytes}

Type: int64

When reading from Kafka, max bytes to fetch per read

## kafka_fetch_wait_max_ms {#kafka_fetch_wait_max_ms}

Type: int64

When reading from Kafka, max wait time

## kafka_max_message_size {#kafka_max_message_size}

Type: uint64

When writing to Kafka, the maximum size of a message in bytes. This setting only works when a row based output format is used. It works together with `max_insert_block_size`, which controls the maximum rows of data one message can contain. When one of these limits is reached, a message will be created.

## kafka_max_wait_ms {#kafka_max_wait_ms}

Type: milliseconds

The wait time for reading from Kafka before retry.

## kafka_schema_registry_ca_location {#kafka_schema_registry_ca_location}

Type: string

Path to the file or directory containing the CA/root certificates.

## kafka_schema_registry_cert_file {#kafka_schema_registry_cert_file}

Type: string

Path to the certificate file (in PEM format). If the private key and the certificate are stored in the same file, this can be empty if kakfa_schema_registry_private_key_file is given.

## kafka_schema_registry_credentials {#kafka_schema_registry_credentials}

Type: string

Credetials to be used to fetch schema from the `kafka_schema_registry_url`, with format '\<username>:\<password>'.

## kafka_schema_registry_private_key_file {#kafka_schema_registry_private_key_file}

Type: string

Path to the private key file used for encryption. Can be empty if no private key file is used.

## kafka_schema_registry_skip_cert_check {#kafka_schema_registry_skip_cert_check}

Type: bool

If set to true, ignore server certificate check result.

## kafka_schema_registry_url {#kafka_schema_registry_url}

Type: uri

For ProtobufSingle format: Kafka Schema Registry URL.

## keep_versions {#keep_versions}

Type: uint64

Control how many versions for each key kept in memory when joining. Used in versioned_kv join

## keep_windows {#keep_windows}

Type: uint64

How many streaming windows to keep from recycling

## key_space_full_scan_threads {#key_space_full_scan_threads}

Type: uint64

Threads to do full scan of the key space of a Mutable stream shard.

## key_space_parallel_full_scan_threshold {#key_space_parallel_full_scan_threshold}

Type: uint64

The approximate threshold of number of keys which enables parallel full scan of a Mutable stream shard

## keys_already_sharded {#keys_already_sharded}

Type: bool

When data is already sharded correctly on file system and if the aggregation is on the sharding key, we can avoid re-shuffle the data. This is different than allow_independent_shard_processing which is leveraging substream processing

## kv_flush_pool_size {#kv_flush_pool_size}

Type: uint64

Total shared thread pool size for flushing key value batches to MutableStream

## legacy_column_name_of_tuple_literal {#legacy_column_name_of_tuple_literal}

Type: bool

List all names of element of large tuple literals in their column names instead of hash. This settings exists only for compatibility reasons. It makes sense to set to 'true', while doing rolling update of cluster from version lower than 21.7 to higher.

## limit {#limit}

Type: uint64

Limit on read rows from the most 'end' result for select query, default 0 means no limit length

## limit_hint {#limit_hint}

Type: uint64

Max block size limit which may be honored by mutable stream. 0 means unlimited

## live_view_heartbeat_interval {#live_view_heartbeat_interval}

Type: seconds

The heartbeat interval in seconds to indicate live query is alive.

## load_balancing {#load_balancing}

Type: loadbalancing

Which replicas (among healthy replicas) to preferably send a query to (on the first attempt) for distributed processing.

## load_balancing_first_offset {#load_balancing_first_offset}

Type: uint64

Which replica to preferably send a query when FIRST_OR_RANDOM load balancing strategy is used.

## load_marks_asynchronously {#load_marks_asynchronously}

Type: bool

Load MergeTree marks asynchronously

## local {#local}

Type: bool

In a distributed env, local=true means query the local data only

## local_filesystem_read_method {#local_filesystem_read_method}

Type: string

Method of reading data from local filesystem, one of: read, pread, mmap, io_uring, pread_threadpool. The 'io_uring' method is experimental and does not work for Log, TinyLog, StripeLog, File, Set and Join, and other tables with append-able files in presence of concurrent reads and writes.

## local_filesystem_read_prefetch {#local_filesystem_read_prefetch}

Type: bool

Should use prefetching when reading data from local filesystem.

## lock_acquire_timeout {#lock_acquire_timeout}

Type: seconds

How long locking request should wait before failing

## log_comment {#log_comment}

Type: string

Log comment into system.query_log table and server log. It can be set to arbitrary string no longer than max_query_size.

## log_formatted_queries {#log_formatted_queries}

Type: bool

Log formatted queries and write the log to the system table.

## log_insert_query {#log_insert_query}

Type: bool

Log insert query to the system table.

## log_processors_profiles {#log_processors_profiles}

Type: bool

Log Processors profile events.

## log_profile_events {#log_profile_events}

Type: bool

Log query performance statistics into the query_log, query_thread_log and query_views_log.

## log_queries {#log_queries}

Type: bool

Log requests and write the log to the system table.

## log_queries_cut_to_length {#log_queries_cut_to_length}

Type: uint64

If query length is greater than specified threshold (in bytes), then cut query when writing to query log. Also limit length of printed query in ordinary text log.

## log_queries_min_query_duration_ms {#log_queries_min_query_duration_ms}

Type: milliseconds

Minimal time for the query to run, to get to the query_log/query_thread_log/query_views_log.

## log_queries_min_type {#log_queries_min_type}

Type: logqueriestype

Minimal type in query_log to log, possible values (from low to high): QUERY_START, QUERY_FINISH, EXCEPTION_BEFORE_START, EXCEPTION_WHILE_PROCESSING.

## log_queries_probability {#log_queries_probability}

Type: float

Log queries with the specified probabality.

## log_query_settings {#log_query_settings}

Type: bool

Log query settings into the query_log.

## log_query_threads {#log_query_threads}

Type: bool

Log query threads into system.query_thread_log table. This setting have effect only when 'log_queries' is true.

## log_query_views {#log_query_views}

Type: bool

Log query dependent views into system.query_views_log table. This setting have effect only when 'log_queries' is true.

## low_cardinality_allow_in_native_format {#low_cardinality_allow_in_native_format}

Type: bool

Use LowCardinality type in Native format. Otherwise, convert LowCardinality columns to ordinary for select query, and convert ordinary columns to required LowCardinality for insert query.

## low_cardinality_max_dictionary_size {#low_cardinality_max_dictionary_size}

Type: uint64

Maximum size (in rows) of shared global dictionary for LowCardinality type.

## low_cardinality_use_single_dictionary_for_part {#low_cardinality_use_single_dictionary_for_part}

Type: bool

low_cardinality type serialization setting. If is true, than will use additional keys when global dictionary overflows. Otherwise, will create several shared dictionaries.

## materialize_ttl_after_modify {#materialize_ttl_after_modify}

Type: bool

Apply TTL for old data, after ALTER MODIFY TTL query

## max_ast_depth {#max_ast_depth}

Type: uint64

Maximum depth of query syntax tree. Checked after parsing.

## max_ast_elements {#max_ast_elements}

Type: uint64

Maximum size of query syntax tree in number of nodes. Checked after parsing.

## max_backup_threads {#max_backup_threads}

Type: uint64

The maximum number of threads to execute a BACKUP or RESTORE request. By default, it is determined automatically.

## max_block_size {#max_block_size}

Type: uint64

Maximum block size for reading

## max_bytes_before_external_group_by {#max_bytes_before_external_group_by}

Type: uint64



## max_bytes_before_external_sort {#max_bytes_before_external_sort}

Type: uint64



## max_bytes_before_remerge_sort {#max_bytes_before_remerge_sort}

Type: uint64

In case of ORDER BY with LIMIT, when memory usage is higher than specified threshold, perform additional steps of merging blocks before final merge to keep just top LIMIT rows.

## max_bytes_in_distinct {#max_bytes_in_distinct}

Type: uint64

Maximum total size of state (in uncompressed bytes) in memory for the execution of DISTINCT.

## max_bytes_in_join {#max_bytes_in_join}

Type: uint64

Maximum size of the hash table for JOIN (in number of bytes in memory).

## max_bytes_in_set {#max_bytes_in_set}

Type: uint64

Maximum size of the set (in bytes in memory) resulting from the execution of the IN section.

## max_bytes_to_read {#max_bytes_to_read}

Type: uint64

Limit on read bytes (after decompression) from the most 'deep' sources. That is, only in the deepest subquery. When reading from a remote server, it is only checked on a remote server.

## max_bytes_to_read_leaf {#max_bytes_to_read_leaf}

Type: uint64

Limit on read bytes (after decompression) on the leaf nodes for distributed queries. Limit is applied for local reads only excluding the final merge stage on the root node.

## max_bytes_to_sort {#max_bytes_to_sort}

Type: uint64



## max_bytes_to_transfer {#max_bytes_to_transfer}

Type: uint64

Maximum size (in uncompressed bytes) of the transmitted external table obtained when the GLOBAL IN/JOIN section is executed.

## max_channels_per_resource_group {#max_channels_per_resource_group}

Type: uint64

Max channels per shared resource group. One streaming query maps to one channel

## max_columns_to_read {#max_columns_to_read}

Type: uint64



## max_compress_block_size {#max_compress_block_size}

Type: uint64

The maximum size of blocks of uncompressed data before compressing for writing to a table.

## max_concurrent_queries_for_all_users {#max_concurrent_queries_for_all_users}

Type: uint64

The maximum number of concurrent requests for all users.

## max_concurrent_queries_for_user {#max_concurrent_queries_for_user}

Type: uint64

The maximum number of concurrent requests per user.

## max_distributed_connections {#max_distributed_connections}

Type: uint64

The maximum number of connections for distributed processing of one query (should be greater than max_threads).

## max_distributed_depth {#max_distributed_depth}

Type: uint64

Maximum distributed query depth

## max_download_buffer_size {#max_download_buffer_size}

Type: uint64

The maximal size of buffer for parallel downloading (e.g. for URL engine) per each thread.

## max_download_threads {#max_download_threads}

Type: maxthreads

The maximum number of threads to download data (e.g. for URL engine).

## max_entries_for_hash_table_stats {#max_entries_for_hash_table_stats}

Type: uint64

How many entries hash table statistics collected during aggregation is allowed to have

## max_events {#max_events}

Type: uint64

Total events to generate for random stream

## max_execution_speed {#max_execution_speed}

Type: uint64

Maximum number of execution rows per second.

## max_execution_speed_bytes {#max_execution_speed_bytes}

Type: uint64

Maximum number of execution bytes per second.

## max_execution_time {#max_execution_time}

Type: seconds



## max_expanded_ast_elements {#max_expanded_ast_elements}

Type: uint64

Maximum size of query syntax tree in number of nodes after expansion of aliases and the asterisk.

## max_fetch_partition_retries_count {#max_fetch_partition_retries_count}

Type: uint64

Amount of retries while fetching partition from another host.

## max_final_threads {#max_final_threads}

Type: uint64

The maximum number of threads to read from table with FINAL.

## max_hot_keys {#max_hot_keys}

Type: uint64

If not zero, it specifies the number of hot keys will kept in-memory when using hybrid hash table. Zero means no limits

## max_http_get_redirects {#max_http_get_redirects}

Type: uint64

Max number of http GET redirects hops allowed. Make sure additional security measures are in place to prevent a malicious server to redirect your requests to unexpected services.

## max_hyperscan_regexp_length {#max_hyperscan_regexp_length}

Type: uint64

Max length of regexp than can be used in hyperscan multi-match functions. Zero means unlimited.

## max_hyperscan_regexp_total_length {#max_hyperscan_regexp_total_length}

Type: uint64

Max total length of all regexps than can be used in hyperscan multi-match functions (per every function). Zero means unlimited.

## max_idempotent_ids {#max_idempotent_ids}

Type: uint64

Maximum idempotent IDs to keep in memory and on disk for idempotent data ingestion

## max_insert_block_bytes {#max_insert_block_bytes}

Type: uint64

The maximum size in bytes of block for insertion, if we control the creation of blocks for insertion.

## max_insert_block_size {#max_insert_block_size}

Type: uint64

The maximum block size for insertion, if we control the creation of blocks for insertion.

## max_insert_delayed_streams_for_parallel_write {#max_insert_delayed_streams_for_parallel_write}

Type: uint64

The maximum number of streams (columns) to delay final part flush. Default - auto (1000 in case of underlying storage supports parallel write, for example S3 and disabled otherwise)

## max_insert_threads {#max_insert_threads}

Type: uint64

The maximum number of threads to execute the INSERT SELECT query. Values 0 or 1 means that INSERT SELECT is not run in parallel. Higher values will lead to higher memory usage. Parallel INSERT SELECT has effect only if the SELECT part is run on parallel, see 'max_threads' setting.

## max_join_range {#max_join_range}

Type: int64

Max join range

## max_joined_block_size_rows {#max_joined_block_size_rows}

Type: uint64

Maximum block size for JOIN result (if join algorithm supports it). 0 means unlimited.

## max_live_view_insert_blocks_before_refresh {#max_live_view_insert_blocks_before_refresh}

Type: uint64

Limit maximum number of inserted blocks after which mergeable blocks are dropped and query is re-executed.

## max_memory_usage {#max_memory_usage}

Type: uint64

Maximum memory usage for processing of single query. Zero means unlimited.

## max_memory_usage_for_user {#max_memory_usage_for_user}

Type: uint64

Maximum memory usage for processing all concurrently running queries for the user. Zero means unlimited.

## max_network_bandwidth {#max_network_bandwidth}

Type: uint64

The maximum speed of data exchange over the network in bytes per second for a query. Zero means unlimited.

## max_network_bandwidth_for_all_users {#max_network_bandwidth_for_all_users}

Type: uint64

The maximum speed of data exchange over the network in bytes per second for all concurrently running queries. Zero means unlimited.

## max_network_bandwidth_for_user {#max_network_bandwidth_for_user}

Type: uint64

The maximum speed of data exchange over the network in bytes per second for all concurrently running user queries. Zero means unlimited.

## max_network_bytes {#max_network_bytes}

Type: uint64

The maximum number of bytes (compressed) to receive or transmit over the network for execution of the query.

## max_number_of_parameters_for_json_values {#max_number_of_parameters_for_json_values}

Type: uint64

Max arguments number of json_values limit

## max_parallel_replicas {#max_parallel_replicas}

Type: uint64

The maximum number of replicas of each shard used when the query is executed. For consistency (to get different parts of the same partition), this option only works for the specified sampling key. The lag of the replicas is not controlled.

## max_parser_depth {#max_parser_depth}

Type: uint64

Maximum parser depth (recursion depth of recursive descend parser).

## max_partitions_per_insert_block {#max_partitions_per_insert_block}

Type: uint64

Limit maximum number of partitions in single INSERTed block. Zero means unlimited. Throw exception if the block contains too many partitions. This setting is a safety threshold, because using large number of partitions is a common misconception.

## max_partitions_to_read {#max_partitions_to_read}

Type: int64

Limit the max number of partitions that can be accessed in one query. \<= 0 means unlimited.

## max_pipeline_depth {#max_pipeline_depth}

Type: uint64



## max_query_size {#max_query_size}

Type: uint64

Which part of the query can be read into RAM for parsing (the remaining data for INSERT, if any, is read later)

## max_read_buffer_size {#max_read_buffer_size}

Type: uint64

The maximum size of the buffer to read from the filesystem.

## max_remote_read_network_bandwidth_for_server {#max_remote_read_network_bandwidth_for_server}

Type: uint64

The maximum speed of data exchange over the network in bytes per second for read. Zero means unlimited. Only has meaning at server startup.

## max_remote_write_network_bandwidth_for_server {#max_remote_write_network_bandwidth_for_server}

Type: uint64

The maximum speed of data exchange over the network in bytes per second for write. Zero means unlimited. Only has meaning at server startup.

## max_replica_delay_for_distributed_queries {#max_replica_delay_for_distributed_queries}

Type: uint64

If set, distributed queries of Replicated tables will choose servers with replication delay in seconds less than the specified value (not inclusive). Zero means do not take delay into account.

## max_replicated_fetches_network_bandwidth_for_server {#max_replicated_fetches_network_bandwidth_for_server}

Type: uint64

The maximum speed of data exchange over the network in bytes per second for replicated fetches. Zero means unlimited. Only has meaning at server startup.

## max_replicated_sends_network_bandwidth_for_server {#max_replicated_sends_network_bandwidth_for_server}

Type: uint64

The maximum speed of data exchange over the network in bytes per second for replicated sends. Zero means unlimited. Only has meaning at server startup.

## max_result_bytes {#max_result_bytes}

Type: uint64

Limit on result size in bytes (uncompressed). Also checked for intermediate data sent from remote servers.

## max_result_rows {#max_result_rows}

Type: uint64

Limit on result size in rows. Also checked for intermediate data sent from remote servers.

## max_rows_in_distinct {#max_rows_in_distinct}

Type: uint64

Maximum number of elements during execution of DISTINCT.

## max_rows_in_join {#max_rows_in_join}

Type: uint64

Maximum size of the hash table for JOIN (in number of rows).

## max_rows_in_set {#max_rows_in_set}

Type: uint64

Maximum size of the set (in number of elements) resulting from the execution of the IN section.

## max_rows_in_set_to_optimize_join {#max_rows_in_set_to_optimize_join}

Type: uint64

Maximal size of the set to filter joined tables by each other row sets before joining. 0 - disable.

## max_rows_to_group_by {#max_rows_to_group_by}

Type: uint64



## max_rows_to_read {#max_rows_to_read}

Type: uint64

Limit on read rows from the most 'deep' sources. That is, only in the deepest subquery. When reading from a remote server, it is only checked on a remote server.

## max_rows_to_read_leaf {#max_rows_to_read_leaf}

Type: uint64

Limit on read rows on the leaf nodes for distributed queries. Limit is applied for local reads only excluding the final merge stage on the root node.

## max_rows_to_sort {#max_rows_to_sort}

Type: uint64



## max_rows_to_transfer {#max_rows_to_transfer}

Type: uint64

Maximum size (in rows) of the transmitted external table obtained when the GLOBAL IN/JOIN section is executed.

## max_size_to_preallocate_for_aggregation {#max_size_to_preallocate_for_aggregation}

Type: uint64

For how many elements it is allowed to preallocate space in all hash tables in total before aggregation

## max_streaming_view_cached_block_bytes {#max_streaming_view_cached_block_bytes}

Type: uint64

Maximum bytes of block cached in streaming view

## max_streaming_view_cached_block_count {#max_streaming_view_cached_block_count}

Type: uint64

Maximum count of block cached in streaming view

## max_streams_for_merge_tree_reading {#max_streams_for_merge_tree_reading}

Type: uint64

If is not zero, limit the number of reading streams for MergeTree table.

## max_streams_multiplier_for_merge_tables {#max_streams_multiplier_for_merge_tables}

Type: float

Ask more streams when reading from Merge table. Streams will be spread across tables that Merge table will use. This allows more even distribution of work across threads and especially helpful when merged tables differ in size.

## max_streams_to_max_threads_ratio {#max_streams_to_max_threads_ratio}

Type: float

Allows you to use more sources than the number of threads - to more evenly distribute work across threads. It is assumed that this is a temporary solution, since it will be possible in the future to make the number of sources equal to the number of threads, but for each source to dynamically select available work for itself.

## max_subquery_depth {#max_subquery_depth}

Type: uint64



## max_temp_data_on_disk_size_for_query {#max_temp_data_on_disk_size_for_query}

Type: uint64

The maximum amount of data consumed by temporary files on disk in bytes for all concurrently running queries. Zero means unlimited.

## max_temp_data_on_disk_size_for_user {#max_temp_data_on_disk_size_for_user}

Type: uint64

The maximum amount of data consumed by temporary files on disk in bytes for all concurrently running user queries. Zero means unlimited.

## max_temporary_columns {#max_temporary_columns}

Type: uint64



## max_temporary_non_const_columns {#max_temporary_non_const_columns}

Type: uint64



## max_threads {#max_threads}

Type: maxthreads

The maximum number of threads to execute the request. By default, it is determined automatically.

## max_untracked_memory {#max_untracked_memory}

Type: uint64

Small allocations and deallocations are grouped in thread local variable and tracked or profiled only when amount (in absolute value) becomes larger than specified value. If the value is higher than 'memory_profiler_step' it will be effectively lowered to 'memory_profiler_step'.

## max_windows {#max_windows}

Type: uint64

Maximum number of streaming windows in one streaming query.

## memory_overcommit_ratio_denominator {#memory_overcommit_ratio_denominator}

Type: uint64

It represents soft memory limit on the user level. This value is used to compute query overcommit ratio.

## memory_overcommit_ratio_denominator_for_user {#memory_overcommit_ratio_denominator_for_user}

Type: uint64

It represents soft memory limit on the global level. This value is used to compute query overcommit ratio.

## memory_profiler_sample_probability {#memory_profiler_sample_probability}

Type: float

Collect random allocations and deallocations and write them into system.trace_log with 'MemorySample' trace_type. The probability is for every alloc/free regardless to the size of the allocation. Note that sampling happens only when the amount of untracked memory exceeds 'max_untracked_memory'. You may want to set 'max_untracked_memory' to 0 for extra fine grained sampling.

## memory_profiler_step {#memory_profiler_step}

Type: uint64

Whenever query memory usage becomes larger than every next step in number of bytes the memory profiler will collect the allocating stack trace. Zero means disabled memory profiler. Values lower than a few megabytes will slow down query processing.

## memory_tracker_fault_probability {#memory_tracker_fault_probability}

Type: float

For testing of `exception safety` - throw an exception every time you allocate memory with the specified probability.

## memory_usage_overcommit_max_wait_microseconds {#memory_usage_overcommit_max_wait_microseconds}

Type: uint64

Maximum time thread will wait for memory to be freed in the case of memory overcommit on user level. If timeout is reached and memory is not freed, exception is thrown.

## memory_weight {#memory_weight}

Type: uint64

Default weight to adjust the leader election timeout

## merge_tree_coarse_index_granularity {#merge_tree_coarse_index_granularity}

Type: uint64

If the index segment can contain the required keys, divide it into as many parts and recursively check them.

## merge_tree_max_bytes_to_use_cache {#merge_tree_max_bytes_to_use_cache}

Type: uint64

The maximum number of bytes per request, to use the cache of uncompressed data. If the request is large, the cache is not used. (For large queries not to flush out the cache.)

## merge_tree_max_rows_to_use_cache {#merge_tree_max_rows_to_use_cache}

Type: uint64

The maximum number of rows per request, to use the cache of uncompressed data. If the request is large, the cache is not used. (For large queries not to flush out the cache.)

## merge_tree_min_bytes_for_concurrent_read {#merge_tree_min_bytes_for_concurrent_read}

Type: uint64

If at least as many bytes are read from one file, the reading can be parallelized.

## merge_tree_min_bytes_for_concurrent_read_for_remote_filesystem {#merge_tree_min_bytes_for_concurrent_read_for_remote_filesystem}

Type: uint64

If at least as many bytes are read from one file, the reading can be parallelized, when reading from remote filesystem.

## merge_tree_min_bytes_for_seek {#merge_tree_min_bytes_for_seek}

Type: uint64

You can skip reading more than that number of bytes at the price of one seek per file.

## merge_tree_min_bytes_per_task_for_remote_reading {#merge_tree_min_bytes_per_task_for_remote_reading}

Type: uint64

Min bytes to read per task.

## merge_tree_min_rows_for_concurrent_read {#merge_tree_min_rows_for_concurrent_read}

Type: uint64

If at least as many lines are read from one file, the reading can be parallelized.

## merge_tree_min_rows_for_concurrent_read_for_remote_filesystem {#merge_tree_min_rows_for_concurrent_read_for_remote_filesystem}

Type: uint64

If at least as many lines are read from one file, the reading can be parallelized, when reading from remote filesystem.

## merge_tree_min_rows_for_seek {#merge_tree_min_rows_for_seek}

Type: uint64

You can skip reading more than that number of rows at the price of one seek per file.

## merge_tree_use_const_size_tasks_for_remote_reading {#merge_tree_use_const_size_tasks_for_remote_reading}

Type: bool

Whether to use constant size tasks for reading from a remote table.

## metrics_perf_events_enabled {#metrics_perf_events_enabled}

Type: bool

If enabled, some of the perf events will be measured throughout queries' execution.

## metrics_perf_events_list {#metrics_perf_events_list}

Type: string

Comma separated list of perf metrics that will be measured throughout queries' execution. Empty means all events. See PerfEventInfo in sources for the available events.

## min_bytes_to_use_direct_io {#min_bytes_to_use_direct_io}

Type: uint64

The minimum number of bytes for reading the data with O_DIRECT option during SELECT queries execution. 0 - disabled.

## min_bytes_to_use_mmap_io {#min_bytes_to_use_mmap_io}

Type: uint64

The minimum number of bytes for reading the data with mmap option during SELECT queries execution. 0 - disabled.

## min_chunk_bytes_for_parallel_parsing {#min_chunk_bytes_for_parallel_parsing}

Type: uint64

The minimum chunk size in bytes, which each thread will parse in parallel.

## min_compress_block_size {#min_compress_block_size}

Type: uint64

The actual size of the block to compress, if the uncompressed data less than max_compress_block_size is no less than this value and no less than the volume of data for one mark.

## min_count_to_compile_aggregate_expression {#min_count_to_compile_aggregate_expression}

Type: uint64

The number of identical aggregate expressions before they are JIT-compiled

## min_count_to_compile_expression {#min_count_to_compile_expression}

Type: uint64

The number of identical expressions before they are JIT-compiled

## min_execution_speed {#min_execution_speed}

Type: uint64

Minimum number of execution rows per second.

## min_execution_speed_bytes {#min_execution_speed_bytes}

Type: uint64

Minimum number of execution bytes per second.

## min_free_disk_space_for_temporary_data {#min_free_disk_space_for_temporary_data}

Type: uint64

The minimum disk space to keep while writing temporary data used in external sorting and aggregation.

## min_hit_rate_to_use_consecutive_keys_optimization {#min_hit_rate_to_use_consecutive_keys_optimization}

Type: float

Minimal hit rate of a cache which is used for consecutive keys optimization in aggregation to keep it enabled

## min_insert_block_size_bytes {#min_insert_block_size_bytes}

Type: uint64

Squash blocks passed to INSERT query to specified size in bytes, if blocks are not big enough.

## min_insert_block_size_bytes_for_materialized_views {#min_insert_block_size_bytes_for_materialized_views}

Type: uint64

Like min_insert_block_size_bytes, but applied only during pushing to MATERIALIZED VIEW (default: min_insert_block_size_bytes)

## min_insert_block_size_rows {#min_insert_block_size_rows}

Type: uint64

Squash blocks passed to INSERT query to specified size in rows, if blocks are not big enough.

## min_insert_block_size_rows_for_materialized_views {#min_insert_block_size_rows_for_materialized_views}

Type: uint64

Like min_insert_block_size_rows, but applied only during pushing to MATERIALIZED VIEW (default: min_insert_block_size_rows)

## min_threads {#min_threads}

Type: uint64

The minimum number of threads to execute the request. By default, it is determined automatically.

## move_all_conditions_to_prewhere {#move_all_conditions_to_prewhere}

Type: bool

Move all viable conditions from WHERE to PREWHERE

## mutations_sync {#mutations_sync}

Type: uint64

Wait for synchronous execution of ALTER TABLE UPDATE/DELETE queries (mutations). 0 - execute asynchronously. 1 - wait current server. 2 - wait all replicas if they exist.

## mv_election_timeout_factor_for_best_candidates {#mv_election_timeout_factor_for_best_candidates}

Type: float

Multiplier to expedite leader election for nodes in desired operational state

## mv_election_timeout_factor_for_fallback_candidates {#mv_election_timeout_factor_for_fallback_candidates}

Type: float

Multiplier applied to fallback node election timeout in internal algorithms

## mv_election_timeout_factor_for_overloaded_candidates {#mv_election_timeout_factor_for_overloaded_candidates}

Type: float

Multiplier to extend leader election timeout for overloaded nodes

## mysql_datatypes_support_level {#mysql_datatypes_support_level}

Type: mysqldatatypessupport

Which MySQL types should be converted to corresponding Timeplusd types (rather than being represented as String). Can be empty or any combination of 'decimal' or 'datetime64'. When empty MySQL's DECIMAL and DATETIME/TIMESTAMP with non-zero precision are seen as String on proton's side.

## mysql_max_rows_to_insert {#mysql_max_rows_to_insert}

Type: uint64

The maximum number of rows in MySQL batch insertion of the MySQL storage engine

## network_compression_method {#network_compression_method}

Type: string

Allows you to select the method of data compression when writing.

## network_zstd_compression_level {#network_zstd_compression_level}

Type: int64

Allows you to select the level of ZSTD compression.

## nlog_adhoc_pool_size {#nlog_adhoc_pool_size}

Type: uint64

Thread pool size for nativelog ad-hoc tasks

## nlog_background_pool_size {#nlog_background_pool_size}

Type: uint64

Thread pool size for nativelog ad-hoc tasks

## normalize_function_names {#normalize_function_names}

Type: bool

Normalize function names to their canonical names

## numpy_optimize_enable {#numpy_optimize_enable}

Type: bool

Using numpy for zero-copy, this will greatly speed up the process 

## odbc_bridge_connection_pool_size {#odbc_bridge_connection_pool_size}

Type: uint64

Connection pool size for each connection settings string in ODBC bridge.

## offset {#offset}

Type: uint64

Offset on read rows from the most 'end' result for select query

## opentelemetry_start_trace_probability {#opentelemetry_start_trace_probability}

Type: float

Probability to start an OpenTelemetry trace for an incoming query.

## opentelemetry_trace_processors {#opentelemetry_trace_processors}

Type: bool

Collect OpenTelemetry spans for processors.

## optimize_aggregation_emit_on_updates {#optimize_aggregation_emit_on_updates}

Type: bool

Optimize aggregation for updates/changes, it may also lead to significantly increased memory usage in extreme scenarios

## optimize_aggregation_in_order {#optimize_aggregation_in_order}

Type: bool

Enable GROUP BY optimization for aggregating data in corresponding order in MergeTree tables.

## optimize_aggregators_of_group_by_keys {#optimize_aggregators_of_group_by_keys}

Type: bool

Eliminates min/max/any/any_last aggregators of GROUP BY keys in SELECT section

## optimize_append_index {#optimize_append_index}

Type: bool

Use constraints in order to append index condition (indexHint)

## optimize_arithmetic_operations_in_aggregate_functions {#optimize_arithmetic_operations_in_aggregate_functions}

Type: bool

Move arithmetic operations out of aggregation functions

## optimize_distinct_in_order {#optimize_distinct_in_order}

Type: bool

Enable DISTINCT optimization if some columns in DISTINCT form a prefix of sorting. For example, prefix of sorting key in merge tree or ORDER BY statement

## optimize_distributed_group_by_sharding_key {#optimize_distributed_group_by_sharding_key}

Type: bool

Optimize GROUP BY sharding_key queries (by avoiding costly aggregation on the initiator server).

## optimize_duplicate_order_by_and_distinct {#optimize_duplicate_order_by_and_distinct}

Type: bool

Remove duplicate ORDER BY and DISTINCT if it's possible

## optimize_functions_to_subcolumns {#optimize_functions_to_subcolumns}

Type: bool

Transform functions to subcolumns, if possible, to reduce amount of read data. E.g. 'length(arr)' -> 'arr.size0', 'col IS NULL' -> 'col.null' 

## optimize_fuse_sum_count_avg {#optimize_fuse_sum_count_avg}

Type: bool

Not ready for production, do not use. Fuse functions `sum, avg, count` with identical arguments into one `sumCount` (`optimize_syntax_fuse_functions should be enabled)

## optimize_group_by_constant_keys {#optimize_group_by_constant_keys}

Type: bool

Optimize GROUP BY when all keys in block are constant

## optimize_group_by_function_keys {#optimize_group_by_function_keys}

Type: bool

Eliminates functions of other keys in GROUP BY section

## optimize_if_chain_to_multiif {#optimize_if_chain_to_multiif}

Type: bool

Replace if(cond1, then1, if(cond2, ...)) chains to multiIf. Currently it's not beneficial for numeric types.

## optimize_if_transform_strings_to_enum {#optimize_if_transform_strings_to_enum}

Type: bool

Replaces string-type arguments in If and Transform to enum. Disabled by default cause it could make inconsistent change in distributed query that would lead to its fail.

## optimize_injective_functions_inside_uniq {#optimize_injective_functions_inside_uniq}

Type: bool

Delete injective functions of one argument inside uniq*() functions.

## optimize_min_equality_disjunction_chain_length {#optimize_min_equality_disjunction_chain_length}

Type: uint64

The minimum length of the expression `expr = x1 OR ... expr = xN` for optimization 

## optimize_monotonous_functions_in_order_by {#optimize_monotonous_functions_in_order_by}

Type: bool

Replace monotonous function with its argument in ORDER BY

## optimize_move_functions_out_of_any {#optimize_move_functions_out_of_any}

Type: bool

Move functions out of aggregate functions 'any', 'any_last'.

## optimize_move_to_prewhere {#optimize_move_to_prewhere}

Type: bool

Allows disabling WHERE to PREWHERE optimization in SELECT queries from MergeTree.

## optimize_move_to_prewhere_if_final {#optimize_move_to_prewhere_if_final}

Type: bool

If query has `FINAL`, the optimization `move_to_prewhere` is not always correct and it is enabled only if both settings `optimize_move_to_prewhere` and `optimize_move_to_prewhere_if_final` are turned on

## optimize_normalize_count_variants {#optimize_normalize_count_variants}

Type: bool

Rewrite aggregate functions that semantically equals to count() as count().

## optimize_on_insert {#optimize_on_insert}

Type: bool

Do the same transformation for inserted block of data as if merge was done on this block.

## optimize_read_in_order {#optimize_read_in_order}

Type: bool

Enable ORDER BY optimization for reading data in corresponding order in MergeTree tables.

## optimize_read_in_window_order {#optimize_read_in_window_order}

Type: bool

Enable ORDER BY optimization in window clause for reading data in corresponding order in MergeTree tables.

## optimize_redundant_functions_in_order_by {#optimize_redundant_functions_in_order_by}

Type: bool

Remove functions from ORDER BY if its argument is also in ORDER BY

## optimize_respect_aliases {#optimize_respect_aliases}

Type: bool

If it is set to true, it will respect aliases in WHERE/GROUP BY/ORDER BY, that will help with partition pruning/secondary indexes/optimize_aggregation_in_order/optimize_read_in_order/optimize_trivial_count

## optimize_rewrite_sum_if_to_count_if {#optimize_rewrite_sum_if_to_count_if}

Type: bool

Rewrite sumIf() and sum(if()) function countIf() function when logically equivalent

## optimize_skip_merged_partitions {#optimize_skip_merged_partitions}

Type: bool

Skip partitions with one part with level > 0 in optimize final

## optimize_skip_unused_shards {#optimize_skip_unused_shards}

Type: bool

Assumes that data is distributed by sharding_key. Optimization to skip unused shards if SELECT query filters by sharding_key.

## optimize_skip_unused_shards_limit {#optimize_skip_unused_shards_limit}

Type: uint64

Limit for number of sharding key values, turns off optimize_skip_unused_shards if the limit is reached

## optimize_skip_unused_shards_nesting {#optimize_skip_unused_shards_nesting}

Type: uint64

Same as optimize_skip_unused_shards, but accept nesting level until which it will work.

## optimize_skip_unused_shards_rewrite_in {#optimize_skip_unused_shards_rewrite_in}

Type: bool

Rewrite IN in query for remote shards to exclude values that does not belong to the shard (requires optimize_skip_unused_shards)

## optimize_sorting_by_input_stream_properties {#optimize_sorting_by_input_stream_properties}

Type: bool

Optimize sorting by sorting properties of input stream

## optimize_substitute_columns {#optimize_substitute_columns}

Type: bool

Use constraints for column substitution

## optimize_syntax_fuse_functions {#optimize_syntax_fuse_functions}

Type: bool

Not ready for production, do not use. Allow apply syntax optimisation: fuse aggregate functions

## optimize_throw_if_noop {#optimize_throw_if_noop}

Type: bool

If setting is enabled and OPTIMIZE query didn't actually assign a merge then an explanatory exception is thrown

## optimize_trivial_count_query {#optimize_trivial_count_query}

Type: bool

Process trivial 'SELECT count() FROM table' query from metadata.

## optimize_trivial_insert_select {#optimize_trivial_insert_select}

Type: bool

Optimize trivial 'INSERT INTO table SELECT ... FROM TABLES' query

## optimize_using_constraints {#optimize_using_constraints}

Type: bool

Use constraints for query optimization

## order_direction_hint {#order_direction_hint}

Type: int64

Scan order for mutable stream. 1 - Ascending order, -1 - Descending order. 0 - System default

## os_thread_priority {#os_thread_priority}

Type: int64

If non zero - set corresponding 'nice' value for query processing threads. Can be used to adjust query priority for OS scheduler.

## output_batch_max_delay_ms {#output_batch_max_delay_ms}

Type: uint64

For external streams that support sending data in batch. Set the max time for message publish delay permitted in a batch.

## output_batch_max_messages {#output_batch_max_messages}

Type: uint64

For external streams that support sending data in batch. Set the max number of messages permitted in a batch. If you set this option to a value greater than 1, messages are queued until this threshold is reached or batch interval has elapsed.

## output_batch_max_size_bytes {#output_batch_max_size_bytes}

Type: uint64

For external streams that support sending data in batch. Set the max size of messages permitted in a batch. If you set this option to a value greater than 1,  messages are queued until this threshold is reached or batch interval has elapsed.

## output_format_arrow_fixed_string_as_fixed_byte_array {#output_format_arrow_fixed_string_as_fixed_byte_array}

Type: bool

Use Arrow FIXED_SIZE_BINARY type instead of Binary for FixedString columns.

## output_format_arrow_low_cardinality_as_dictionary {#output_format_arrow_low_cardinality_as_dictionary}

Type: bool

Enable output LowCardinality type as Dictionary Arrow type

## output_format_arrow_string_as_string {#output_format_arrow_string_as_string}

Type: bool

Use Arrow String type instead of Binary for String columns

## output_format_avro_codec {#output_format_avro_codec}

Type: string

Compression codec used for output. Possible values: 'null', 'deflate', 'snappy'.

## output_format_avro_rows_in_file {#output_format_avro_rows_in_file}

Type: uint64

Max rows in a file (if permitted by storage)

## output_format_avro_string_column_pattern {#output_format_avro_string_column_pattern}

Type: string

For Avro format: regexp of String columns to select as AVRO string.

## output_format_avro_sync_interval {#output_format_avro_sync_interval}

Type: uint64

Sync interval in bytes.

## output_format_csv_crlf_end_of_line {#output_format_csv_crlf_end_of_line}

Type: bool

If it is set true, end of line in CSV format will be \r\n instead of \n.

## output_format_decimal_trailing_zeros {#output_format_decimal_trailing_zeros}

Type: bool

Output trailing zeros when printing Decimal values. E.g. 1.230000 instead of 1.23.

## output_format_enable_streaming {#output_format_enable_streaming}

Type: bool

Enable streaming in output formats that support it.

## output_format_json_array_of_rows {#output_format_json_array_of_rows}

Type: bool

Output a JSON array of all rows in JSONEachRow(Compact) format.

## output_format_json_escape_forward_slashes {#output_format_json_escape_forward_slashes}

Type: bool

Controls escaping forward slashes for string outputs in JSON output format. This is intended for compatibility with JavaScript. Don't confuse with backslashes that are always escaped.

## output_format_json_named_tuples_as_objects {#output_format_json_named_tuples_as_objects}

Type: bool

Serialize named tuple columns as JSON objects.

## output_format_json_quote_64bit_integers {#output_format_json_quote_64bit_integers}

Type: bool

Controls quoting of 64-bit integers in JSON output format.

## output_format_json_quote_denormals {#output_format_json_quote_denormals}

Type: bool

Enables '+nan', '-nan', '+inf', '-inf' outputs in JSON output format.

## output_format_orc_string_as_string {#output_format_orc_string_as_string}

Type: bool

Use ORC String type instead of Binary for String columns

## output_format_parallel_formatting {#output_format_parallel_formatting}

Type: bool

Enable parallel formatting for some data formats.

## output_format_parquet_fixed_string_as_fixed_byte_array {#output_format_parquet_fixed_string_as_fixed_byte_array}

Type: bool

Use Parquet FIXED_LENGTH_BYTE_ARRAY type instead of Binary for FixedString columns.

## output_format_parquet_row_group_size {#output_format_parquet_row_group_size}

Type: uint64

Row group size in rows.

## output_format_parquet_string_as_string {#output_format_parquet_string_as_string}

Type: bool

Use Parquet String type instead of Binary for String columns.

## output_format_pretty_color {#output_format_pretty_color}

Type: bool

Use ANSI escape sequences to paint colors in Pretty formats

## output_format_pretty_grid_charset {#output_format_pretty_grid_charset}

Type: string

Charset for printing grid borders. Available charsets: ASCII, UTF-8 (default one).

## output_format_pretty_max_column_pad_width {#output_format_pretty_max_column_pad_width}

Type: uint64

Maximum width to pad all values in a column in Pretty formats.

## output_format_pretty_max_rows {#output_format_pretty_max_rows}

Type: uint64

Rows limit for Pretty formats.

## output_format_pretty_max_value_width {#output_format_pretty_max_value_width}

Type: uint64

Maximum width of value to display in Pretty formats. If greater - it will be cut.

## output_format_pretty_row_numbers {#output_format_pretty_row_numbers}

Type: bool

Add row numbers before each row for pretty output format

## output_format_protobuf_nullables_with_google_wrappers {#output_format_protobuf_nullables_with_google_wrappers}

Type: bool

When serializing Nullable columns with Google wrappers, serialize default values as empty wrappers. If turned off, default and null values are not serialized

## output_format_schema {#output_format_schema}

Type: string

The path to the file where the automatically generated schema will be saved

## output_format_tsv_crlf_end_of_line {#output_format_tsv_crlf_end_of_line}

Type: bool

If it is set true, end of line in TSV format will be \r\n instead of \n.

## output_format_write_statistics {#output_format_write_statistics}

Type: bool

Write statistics about read rows, bytes, time elapsed in suitable output formats.

## parallel_distributed_insert_select {#parallel_distributed_insert_select}

Type: uint64

Process distributed INSERT SELECT query in the same cluster on local tables on every shard, if 1 SELECT is executed on each shard, if 2 SELECT and INSERT is executed on each shard

## parallel_replica_offset {#parallel_replica_offset}

Type: uint64



## parallel_replicas_count {#parallel_replicas_count}

Type: uint64



## parallel_replicas_custom_key {#parallel_replicas_custom_key}

Type: string

Custom key assigning work to replicas when parallel replicas are used.

## parallel_replicas_custom_key_filter_type {#parallel_replicas_custom_key_filter_type}

Type: parallelreplicascustomkeyfiltertype

Type of filter to use with custom key for parallel replicas. default - use modulo operation on the custom key, range - use range filter on custom key using all possible values for the value type of custom key.

## parallel_replicas_single_task_marks_count_multiplier {#parallel_replicas_single_task_marks_count_multiplier}

Type: float

A multiplier which will be added during calculation for minimal number of marks to retrieve from coordinator. This will be applied only for remote replicas.

## parallel_view_processing {#parallel_view_processing}

Type: bool

Enables pushing to attached views concurrently instead of sequentially.

## part_commit_pool_size {#part_commit_pool_size}

Type: uint64

Total shared thread pool size for building and committing parts for Stream

## partial_merge_join_left_table_buffer_bytes {#partial_merge_join_left_table_buffer_bytes}

Type: uint64

If not 0 group left table blocks in bigger ones for left-side table in partial merge join. It uses up to 2x of specified memory per joining thread.

## partial_merge_join_rows_in_right_blocks {#partial_merge_join_rows_in_right_blocks}

Type: uint64

Split right-hand joining data in blocks of specified size. It's a portion of data indexed by min-max values and possibly unloaded on disk.

## pause_on_start {#pause_on_start}

Type: bool

Pause the query on start for materialized view

## periodic_live_view_refresh {#periodic_live_view_refresh}

Type: seconds

Interval after which periodically refreshed live view is forced to refresh.

## poll_interval {#poll_interval}

Type: uint64

Block at the query wait loop on the server for the specified number of seconds.

## postgresql_connection_pool_size {#postgresql_connection_pool_size}

Type: uint64

Connection pool size for PostgreSQL table engine and database engine.

## postgresql_connection_pool_wait_timeout {#postgresql_connection_pool_wait_timeout}

Type: uint64

Connection pool push/pop timeout on empty pool for PostgreSQL table engine and database engine. By default it will block on empty pool.

## prefer_column_name_to_alias {#prefer_column_name_to_alias}

Type: bool

Prefer using column names instead of aliases if possible.

## prefer_global_in_and_join {#prefer_global_in_and_join}

Type: bool

If enabled, all IN/JOIN operators will be rewritten as GLOBAL IN/JOIN. It's useful when the to-be-joined tables are only available on the initiator and we need to always scatter their data on-the-fly during distributed processing with the GLOBAL keyword. It's also useful to reduce the need to access the external sources joining external tables.

## prefer_localhost_replica {#prefer_localhost_replica}

Type: bool

If it's true then queries will be always sent to local replica (if it exists). If it's false then replica to send a query will be chosen between local and remote ones according to load_balancing

## preferred_block_size_bytes {#preferred_block_size_bytes}

Type: uint64



## preferred_max_column_in_block_size_bytes {#preferred_max_column_in_block_size_bytes}

Type: uint64

Limit on max column size in block while reading. Helps to decrease cache misses count. Should be close to L2 cache size.

## prefetch_buffer_size {#prefetch_buffer_size}

Type: uint64

The maximum size of the prefetch buffer to read from the filesystem.

## priority {#priority}

Type: uint64

Priority of the query. 1 - the highest, higher value - lower priority; 0 - do not use priorities.

## pulsar_max_pending_messages {#pulsar_max_pending_messages}

Type: uint64

Set the max size of the producer's queue holding the messages pending to receive an acknowledgment from the broker. When the queue is full, the producer will be blocked.

## push_down_predicate {#push_down_predicate}

Type: bool

Apply the predicates at source reader layer.

## query_mode {#query_mode}

Type: string

Default query mode. table or streaming

## query_plan_aggregation_in_order {#query_plan_aggregation_in_order}

Type: bool

Use query plan for aggregation-in-order optimisation

## query_plan_convert_outer_join_to_inner_join {#query_plan_convert_outer_join_to_inner_join}

Type: bool

Allow to convert OUTER JOIN to INNER JOIN if filter after JOIN always filters default values

## query_plan_enable_optimizations {#query_plan_enable_optimizations}

Type: bool

Apply optimizations to query plan

## query_plan_filter_push_down {#query_plan_filter_push_down}

Type: bool

Allow to push down filter by predicate query plan step

## query_plan_max_optimizations_to_apply {#query_plan_max_optimizations_to_apply}

Type: uint64

Limit the total number of optimizations applied to query plan. If zero, ignored. If limit reached, throw exception

## query_plan_optimize_primary_key {#query_plan_optimize_primary_key}

Type: bool

Analyze primary key using query plan (instead of AST)

## query_plan_read_in_order {#query_plan_read_in_order}

Type: bool

Use query plan for read-in-order optimisation

## query_plan_remove_redundant_sorting {#query_plan_remove_redundant_sorting}

Type: bool

Remove redundant sorting in query plan. For example, sorting steps related to ORDER BY clauses in subqueries

## query_profiler_cpu_time_period_ns {#query_profiler_cpu_time_period_ns}

Type: uint64

Period for CPU clock timer of query profiler (in nanoseconds). Set 0 value to turn off the CPU clock query profiler. Recommended value is at least 10000000 (100 times a second) for single queries or 1000000000 (once a second) for cluster-wide profiling.

## query_profiler_real_time_period_ns {#query_profiler_real_time_period_ns}

Type: uint64

Period for real clock timer of query profiler (in nanoseconds). Set 0 value to turn off the real clock query profiler. Recommended value is at least 10000000 (100 times a second) for single queries or 1000000000 (once a second) for cluster-wide profiling.

## query_resource_group {#query_resource_group}

Type: string

Default resource group. 'dedicated' or 'shared'

## queue_max_wait_ms {#queue_max_wait_ms}

Type: milliseconds

The wait time in the request queue, if the number of concurrent requests exceeds the maximum.

## rabbitmq_max_wait_ms {#rabbitmq_max_wait_ms}

Type: milliseconds

The wait time for reading from RabbitMQ before retry.

## rawstore_time_extraction_rule {#rawstore_time_extraction_rule}

Type: string

_tp_time extraction rule (string, json, regex)

## rawstore_time_extraction_type {#rawstore_time_extraction_type}

Type: string

_tp_time extraction type (string, json, regex)

## read_backoff_max_throughput {#read_backoff_max_throughput}

Type: uint64

Settings to reduce the number of threads in case of slow reads. Count events when the read bandwidth is less than that many bytes per second.

## read_backoff_min_concurrency {#read_backoff_min_concurrency}

Type: uint64

Settings to try keeping the minimal number of threads in case of slow reads.

## read_backoff_min_events {#read_backoff_min_events}

Type: uint64

Settings to reduce the number of threads in case of slow reads. The number of events after which the number of threads will be reduced.

## read_backoff_min_interval_between_events_ms {#read_backoff_min_interval_between_events_ms}

Type: milliseconds

Settings to reduce the number of threads in case of slow reads. Do not pay attention to the event, if the previous one has passed less than a certain amount of time.

## read_backoff_min_latency_ms {#read_backoff_min_latency_ms}

Type: milliseconds

Setting to reduce the number of threads in case of slow reads. Pay attention only to reads that took at least that much time.

## read_from_filesystem_cache_if_exists_otherwise_bypass_cache {#read_from_filesystem_cache_if_exists_otherwise_bypass_cache}

Type: bool



## read_in_order_two_level_merge_threshold {#read_in_order_two_level_merge_threshold}

Type: uint64

Minimal number of parts to read to run preliminary merge step during multithread reading in order of primary key.

## read_overflow_mode {#read_overflow_mode}

Type: overflowmode

What to do when the limit is exceeded.

## read_overflow_mode_leaf {#read_overflow_mode_leaf}

Type: overflowmode

What to do when the leaf limit is exceeded.

## read_priority {#read_priority}

Type: int64

Priority to read data from local filesystem. Only supported for 'pread_threadpool' method.

## readonly {#readonly}

Type: uint64

0 - everything is allowed. 1 - only read requests. 2 - only read requests, as well as changing settings, except for the 'readonly' setting.

## receive_data_timeout_ms {#receive_data_timeout_ms}

Type: milliseconds

Connection timeout for receiving first packet of data or packet with positive progress from replica

## receive_timeout {#receive_timeout}

Type: seconds



## record_consume_batch_count {#record_consume_batch_count}

Type: uint64

Maximum number for consuming records at once

## record_consume_batch_size {#record_consume_batch_size}

Type: uint64

Maximum size for consuming records at once, in bytes

## record_consume_timeout_ms {#record_consume_timeout_ms}

Type: int64

Timeout of consuming record

## recovery_policy {#recovery_policy}

Type: recoverypolicy

Default recovery policy for materialized view when inner query failed. 'strict': always recover from checkpointed; 'best_effort': attempts to recover from checkpointed and allow skipping of some data with permanent errors;

## recovery_retry_for_same_error {#recovery_retry_for_same_error}

Type: uint64

Maximum retry times for same error for materialized view. only apply for 'best_effort'

## regexp_max_matches_per_row {#regexp_max_matches_per_row}

Type: uint64

Max matches of any single regexp per row, used to safeguard 'extractAllGroupsHorizontal' against consuming too much memory with greedy RE.

## remerge_sort_lowered_memory_bytes_ratio {#remerge_sort_lowered_memory_bytes_ratio}

Type: float

If memory usage after remerge does not reduced by this ratio, remerge will be disabled.

## remote_fetch {#remote_fetch}

Type: bool

Control if the current query is a remote fetch query, only used internally

## remote_filesystem_read_method {#remote_filesystem_read_method}

Type: string

Method of reading data from remote filesystem, one of: read, threadpool.

## remote_filesystem_read_prefetch {#remote_filesystem_read_prefetch}

Type: bool

Should use prefetching when reading data from remote filesystem.

## remote_fs_read_backoff_max_tries {#remote_fs_read_backoff_max_tries}

Type: int64

Max attempts to read with backoff

## remote_fs_read_max_backoff_ms {#remote_fs_read_max_backoff_ms}

Type: int64

Max wait time when trying to read data for remote disk

## remote_read_min_bytes_for_seek {#remote_read_min_bytes_for_seek}

Type: uint64

Min bytes required for remote read (url, s3) to do seek, instead of read with ignore.

## replace_running_query {#replace_running_query}

Type: bool

Whether the running request should be canceled with the same id as the new one.

## replace_running_query_max_wait_ms {#replace_running_query_max_wait_ms}

Type: milliseconds

The wait time for running query with the same query_id to finish when setting 'replace_running_query' is active.

## replay_speed {#replay_speed}

Type: float

Control the replay speed..0 \< replay_speed \< 1, means replay slower.replay_speed == 1, means replay by actual ingest interval.1 \< replay_speed \< \<max_limit>, means replay faster

## replay_time_column {#replay_time_column}

Type: string

user specified replay time column, default column is _tp_append_time

## replication_alter_partitions_sync {#replication_alter_partitions_sync}

Type: uint64

Wait for actions to manipulate the partitions. 0 - do not wait, 1 - wait for execution only of itself, 2 - wait for everyone.

## replication_wait_for_inactive_replica_timeout {#replication_wait_for_inactive_replica_timeout}

Type: int64

Wait for inactive replica to execute ALTER/OPTIMIZE. Time in seconds, 0 - do not wait, negative - wait for unlimited time.

## result_overflow_mode {#result_overflow_mode}

Type: overflowmode

What to do when the limit is exceeded.

## retract_k_multiplier {#retract_k_multiplier}

Type: uint64

Control how many more values to keep around for changelog processing to workaround retract scenarios in min_k/max_k etc aggr

## retract_max {#retract_max}

Type: uint64

Control how many more values to keep around for changelog processing to workaround retract scenarios in min/max etc aggr

## s3_check_objects_after_upload {#s3_check_objects_after_upload}

Type: bool

Check each uploaded object to s3 with head request to be sure that upload was successful

## s3_create_new_file_on_insert {#s3_create_new_file_on_insert}

Type: bool

Enables or disables creating a new file on each insert in s3 engine tables

## s3_max_connections {#s3_max_connections}

Type: uint64

The maximum number of connections per server.

## s3_max_get_burst {#s3_max_get_burst}

Type: uint64

Max number of requests that can be issued simultaneously before hitting request per second limit. By default (0) equals to `s3_max_get_rps`

## s3_max_get_rps {#s3_max_get_rps}

Type: uint64

Limit on S3 GET request per second rate before throttling. Zero means unlimited.

## s3_max_put_burst {#s3_max_put_burst}

Type: uint64

Max number of requests that can be issued simultaneously before hitting request per second limit. By default (0) equals to `s3_max_put_rps`

## s3_max_put_rps {#s3_max_put_rps}

Type: uint64

Limit on S3 PUT request per second rate before throttling. Zero means unlimited.

## s3_max_redirects {#s3_max_redirects}

Type: uint64

Max number of S3 redirects hops allowed.

## s3_max_single_part_upload_size {#s3_max_single_part_upload_size}

Type: uint64

The maximum size of object to upload using singlepart upload to S3.

## s3_max_single_read_retries {#s3_max_single_read_retries}

Type: uint64

The maximum number of retries during single S3 read.

## s3_max_unexpected_write_error_retries {#s3_max_unexpected_write_error_retries}

Type: uint64

The maximum number of retries in case of unexpected errors during S3 write.

## s3_max_upload_idle_seconds {#s3_max_upload_idle_seconds}

Type: uint64

The maximum idle time (in seconds) to wait for new data before complete a upload to S3, 0 means no limits.

## s3_max_upload_part_size {#s3_max_upload_part_size}

Type: uint64

The maximum size of part to upload during multipart upload to S3.

## s3_min_upload_file_size {#s3_min_upload_file_size}

Type: uint64

The minimum size of file to upload to S3, i.e. once the file reaches this size, the multipart upload will finish, or the file will be uploaded via single part upload if this size is smaller than s3_max_single_part_upload_size.

## s3_min_upload_part_size {#s3_min_upload_part_size}

Type: uint64

The minimum size of part to upload during multipart upload to S3.

## s3_truncate_on_insert {#s3_truncate_on_insert}

Type: bool

Enables or disables truncate before insert in s3 engine tables.

## s3_upload_part_size_multiply_factor {#s3_upload_part_size_multiply_factor}

Type: uint64

Multiply s3_min_upload_part_size by this factor each time s3_multiply_parts_count_threshold parts were uploaded from a single write to S3.

## s3_upload_part_size_multiply_parts_count_threshold {#s3_upload_part_size_multiply_parts_count_threshold}

Type: uint64

Each time this number of parts was uploaded to S3 s3_min_upload_part_size multiplied by s3_upload_part_size_multiply_factor.

## schema_inference_cache_require_modification_time_for_url {#schema_inference_cache_require_modification_time_for_url}

Type: bool

Use schema from cache for URL with last modification time validation (for urls with Last-Modified header)

## schema_inference_use_cache_for_file {#schema_inference_use_cache_for_file}

Type: bool

Use cache in schema inference while using file table function

## schema_inference_use_cache_for_hdfs {#schema_inference_use_cache_for_hdfs}

Type: bool

Use cache in schema inference while using hdfs table function

## schema_inference_use_cache_for_s3 {#schema_inference_use_cache_for_s3}

Type: bool

Use cache in schema inference while using s3 table function

## schema_inference_use_cache_for_url {#schema_inference_use_cache_for_url}

Type: bool

Use cache in schema inference while using url table function

## seek_to {#seek_to}

Type: string

Seeking to an offset of the streaming/historical store to seek

## select_sequential_consistency {#select_sequential_consistency}

Type: uint64

For SELECT queries from the replicated table, throw an exception if the replica does not have a chunk written with the quorum; do not read the parts that have not yet been written with the quorum.

## send_progress_in_http_body {#send_progress_in_http_body}

Type: bool

Send progress notifications in JSON via HTTP body before sending data

## send_progress_in_http_headers {#send_progress_in_http_headers}

Type: bool

Send progress notifications using X-Proton-Progress headers. Some clients do not support high amount of HTTP headers (Python requests in particular), so it is disabled by default.

## send_timeout {#send_timeout}

Type: seconds



## set_overflow_mode {#set_overflow_mode}

Type: overflowmode

What to do when the limit is exceeded.

## shards {#shards}

Type: string

If not empty, only the specified shard IDs (or partition IDs if the target stream is a Kafka external stream) will be selected to be read data from. IDs are separated by comma. Example: shards='0,2'

## short_circuit_function_evaluation {#short_circuit_function_evaluation}

Type: shortcircuitfunctionevaluation

Setting for short-circuit function evaluation configuration. Possible values: 'enable' - use short-circuit function evaluation for functions that are suitable for it, 'disable' - disable short-circuit function evaluation, 'force_enable' - use short-circuit function evaluation for all functions.

## show_uuid {#show_uuid}

Type: bool

For tables in databases with Engine=Atomic show UUID of the table in its CREATE query.

## skip_download_if_exceeds_query_cache {#skip_download_if_exceeds_query_cache}

Type: bool

Skip download from remote filesystem if exceeds query cache size

## skip_unavailable_shards {#skip_unavailable_shards}

Type: bool

If true, timeplusd silently skips unavailable shards and nodes unresolvable through DNS. Shard is marked as unavailable when none of the replicas can be reached.

## sleep_after_receiving_query_ms {#sleep_after_receiving_query_ms}

Type: milliseconds

Time to sleep after receiving query in TCPHandler

## sleep_in_receive_cancel_ms {#sleep_in_receive_cancel_ms}

Type: milliseconds

Time to sleep in receiving cancel in TCPHandler

## sleep_in_send_data_ms {#sleep_in_send_data_ms}

Type: milliseconds

Time to sleep in sending data in TCPHandler

## sleep_in_send_tables_status_ms {#sleep_in_send_tables_status_ms}

Type: milliseconds

Time to sleep in sending tables status response in TCPHandler

## snapshot_scan_threads {#snapshot_scan_threads}

Type: uint64

Number of threads to scan data while reading snapshot

## sort_overflow_mode {#sort_overflow_mode}

Type: overflowmode

What to do when the limit is exceeded.

## splitby_max_substrings_includes_remaining_string {#splitby_max_substrings_includes_remaining_string}

Type: bool

Functions 'splitBy*()' with 'max_substrings' argument > 0 include the remaining string as last element in the result

## stream_flush_interval_ms {#stream_flush_interval_ms}

Type: milliseconds

Timeout for flushing data from streaming storages.

## stream_like_engine_allow_direct_select {#stream_like_engine_allow_direct_select}

Type: bool

Allow direct SELECT query for Kafka, RabbitMQ and FileLog engines. In case there are attached materialized views, SELECT query is not allowed even if this setting is enabled.

## stream_poll_timeout_ms {#stream_poll_timeout_ms}

Type: milliseconds

Timeout for polling data from/to streaming storages.

## synchronous_ddl {#synchronous_ddl}

Type: bool

If setting is enabled, the DDL for streaming storage will be executed synchronously otherwise it will be asynchronous. By default is enabled.

## system_events_show_zero_values {#system_events_show_zero_values}

Type: bool

Include all metrics, even with zero values

## table_function_remote_max_addresses {#table_function_remote_max_addresses}

Type: uint64

The maximum number of different shards and the maximum number of replicas of one shard in the `remote` function.

## tcp_keep_alive_timeout {#tcp_keep_alive_timeout}

Type: seconds

The time in seconds the connection needs to remain idle before TCP starts sending keepalive probes

## temporary_files_codec {#temporary_files_codec}

Type: string

Set compression codec for temporary files (sort and join on disk). I.e. LZ4, NONE.

## throw_on_error_from_cache_on_write_operations {#throw_on_error_from_cache_on_write_operations}

Type: bool

Ignore error from cache when caching on write operations (INSERT, merges)

## throw_on_unsupported_query_inside_transaction {#throw_on_unsupported_query_inside_transaction}

Type: bool

Throw exception if unsupported query is used inside transaction

## timeout_before_checking_execution_speed {#timeout_before_checking_execution_speed}

Type: seconds

Check that the speed is not too low after the specified time has elapsed.

## timeout_overflow_mode {#timeout_overflow_mode}

Type: overflowmode

What to do when the limit is exceeded.

## totals_auto_threshold {#totals_auto_threshold}

Type: float

The threshold for totals_mode = 'auto'.

## totals_mode {#totals_mode}

Type: totalsmode

How to calculate TOTALS when HAVING is present, as well as when max_rows_to_group_by and group_by_overflow_mode = any are present.

## transfer_overflow_mode {#transfer_overflow_mode}

Type: overflowmode

What to do when the limit is exceeded.

## transform_null_in {#transform_null_in}

Type: bool

If enabled, NULL values will be matched with 'IN' operator as if they are considered equal.

## trival_count_fallback_threshold {#trival_count_fallback_threshold}

Type: uint64

When approximate count reaches this threshold, fallback to regular couting of rows for mutable stream.

## union_default_mode {#union_default_mode}

Type: unionmode

Set default Union Mode in SelectWithUnion query. Possible values: empty string, 'ALL', 'DISTINCT'. If empty, query without Union Mode will throw exception.

## unknown_packet_in_send_data {#unknown_packet_in_send_data}

Type: uint64

Send unknown packet instead of data Nth data packet

## unnest_subqueries {#unnest_subqueries}

Type: bool

Unnest trivial sub queries

## use_approximate_count {#use_approximate_count}

Type: bool

Approximate unique key count for mutable stream.

## use_client_time_zone {#use_client_time_zone}

Type: bool

Use client timezone for interpreting DateTime string values, instead of adopting server timezone.

## use_compact_format_in_distributed_parts_names {#use_compact_format_in_distributed_parts_names}

Type: bool

Changes format of directories names for distributed table insert parts.

## use_hedged_requests {#use_hedged_requests}

Type: bool

Use hedged requests for distributed queries

## use_index {#use_index}

Type: string

Index name. Tell query to apply which index for mutable stream query.

## use_index_for_in_with_subqueries {#use_index_for_in_with_subqueries}

Type: bool

Try using an index if there is a subquery or a table expression on the right side of the IN operator.

## use_index_for_in_with_subqueries_max_values {#use_index_for_in_with_subqueries_max_values}

Type: uint64

The maximum size of set in the right hand side of the IN operator to use table index for filtering. It allows to avoid performance degradation and higher memory usage due to preparation of additional data structures for large queries. Zero means no limit.

## use_local_cache_for_remote_storage {#use_local_cache_for_remote_storage}

Type: bool

Use local cache for remote storage like HDFS or S3, it's used for remote table engine only

## use_skip_indexes {#use_skip_indexes}

Type: bool

Use data skipping indexes during query execution.

## use_skip_indexes_if_final {#use_skip_indexes_if_final}

Type: bool

If query has FINAL, then skipping data based on indexes may produce incorrect result, hence disabled by default.

## use_uncompressed_cache {#use_uncompressed_cache}

Type: bool

Whether to use the cache of uncompressed blocks.

## validate_polygons {#validate_polygons}

Type: bool

Throw exception if polygon is invalid in function pointInPolygon (e.g. self-tangent, self-intersecting). If the setting is false, the function will accept invalid polygons but may silently return wrong result.

## wait_changes_become_visible_after_commit_mode {#wait_changes_become_visible_after_commit_mode}

Type: transactionswaitcsnmode

Wait for committed changes to become actually visible in the latest snapshot

## wait_for_async_insert {#wait_for_async_insert}

Type: bool

If true wait for processing of asynchronous insertion

## wait_for_async_insert_timeout {#wait_for_async_insert_timeout}

Type: seconds

Timeout for waiting for processing asynchronous insertion

## workload_rebalance_check_interval {#workload_rebalance_check_interval}

Type: int64

Interval in milliseconds for performing workload rebalance checks. set workload_rebalance_check_interval \< 0 to disable workload rebalancer

## workload_rebalance_heavy_mv_memory_util_threshold {#workload_rebalance_heavy_mv_memory_util_threshold}

Type: float

Memory usage ratio to consider a MV as a heavy one

## workload_rebalance_overloaded_memory_util_threshold {#workload_rebalance_overloaded_memory_util_threshold}

Type: float

Memory usage ratio to consider a node as overloaded due to other usages.

## zstd_window_log_max {#zstd_window_log_max}

Type: int64

Allows you to select the max window log of ZSTD (it will not be used for MergeTree family)


